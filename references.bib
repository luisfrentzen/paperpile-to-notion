@ARTICLE{Ren2024-dp,
  title         = "Samba: Simple hybrid state space models for efficient
                   unlimited context language modeling",
  author        = "Ren, Liliang and Liu, Yang and Lu, Yadong and Shen, Yelong
                   and Liang, Chen and Chen, Weizhu",
  journal       = "ArXiv",
  volume        = "abs/2406.07522",
  abstract      = "Efficiently modeling sequences with infinite context length
                   has been a long-standing problem. Past works suffer from
                   either the quadratic computation complexity or the limited
                   extrapolation ability on length generalization. In this work,
                   we present Samba, a simple hybrid architecture that
                   layer-wise combines Mamba, a selective State Space Model
                   (SSM), with Sliding Window Attention (SWA). Samba selectively
                   compresses a given sequence into recurrent hidden states
                   while still maintaining the ability to precisely recall
                   memories with the attention mechanism. We scale Samba up to
                   3.8B parameters with 3.2T training tokens and show that Samba
                   substantially outperforms the state-of-the-art models based
                   on pure attention or SSMs on a wide range of benchmarks. When
                   trained on 4K length sequences, Samba can be efficiently
                   extrapolated to 256K context length with perfect memory
                   recall and show improved token predictions up to 1M context
                   length. As a linear-time sequence model, Samba enjoys a 3.73x
                   higher throughput compared to Transformers with grouped-query
                   attention when processing user prompts of 128K length, and
                   3.64x speedup when generating 64K tokens with unlimited
                   streaming. A sample implementation of Samba is publicly
                   available in https://github.com/microsoft/Samba.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  language      = "en"
}

@ARTICLE{Ungless2023-xb,
  title     = "Potential pitfalls with automatic sentiment analysis: The example
               of queerphobic bias",
  author    = "Ungless, Eddie L and Ross, Bj{\"{o}}rn and Belle, Vaishak",
  journal   = "Soc. Sci. Comput. Rev.",
  publisher = "SAGE Publications",
  volume    =  41,
  number    =  6,
  pages     = "2211--2229",
  abstract  = "Automated sentiment analysis can help efficiently detect trends
               in patients' moods, consumer preferences, political attitudes and
               more. Unfortunately, like many natural language processing
               techniques, sentiment analysis can show bias against marginalised
               groups. We illustrate this point by showing how six popular
               sentiment analysis tools respond to sentences about queer
               identities, expanding on existing work on gender, ethnicity and
               disability. We find evidence of bias against several marginalised
               queer identities, including in the two models from Google and
               Amazon that seem to have been subject to superficial debiasing.
               We conclude with guidance on selecting a sentiment analysis tool
               to minimise the risk of model bias skewing results.",
  month     =  dec,
  year      =  2023,
  keywords  = "AI bias; natural language processing; queerphobia; sentiment
               analysis",
  language  = "en"
}

@INCOLLECTION{An2024-lo,
  title     = "Eliminating contextual bias in aspect-based sentiment analysis",
  author    = "An, Ruize and Zhang, Chen and Song, Dawei",
  editor    = "Goharian, Nazli and Tonellotto, Nicola and He, Yulan and Lipani,
               Aldo and McDonald, Graham and Macdonald, Craig and Ounis, Iadh",
  booktitle = "Lecture Notes in Computer Science",
  publisher = "Springer Nature Switzerland",
  address   = "Cham",
  volume    =  14608,
  pages     = "90--107",
  series    = "Lecture notes in computer science",
  year      =  2024,
  language  = "en"
}

@MISC{Maas_undated-ng,
  title        = "Learning word vectors for sentiment analysis",
  author       = "Maas, Andrew L and Daly, Raymond E and Pham, Peter T and
                  Huang, Dan and Ng, Andrew Y and Potts, Christopher",
  howpublished = "\url{https://ai.stanford.edu/~amaas/papers/wvSent\_acl2011.pdf}",
  note         = "Accessed: 2024-12-13"
}

@ARTICLE{UnknownUnknown-ot,
  title    = "interpreting {GPT}: the logit lens",
  abstract = "This post relates an observation I've made in my work with GPT-2,
              which I have not seen made elsewhere. \ldots{}"
}

@INPROCEEDINGS{Kotzias2015-nn,
  title     = "From group to individual labels using deep features",
  author    = "Kotzias, Dimitrios and Denil, Misha and de Freitas, Nando and
               Smyth, Padhraic",
  booktitle = "Proceedings of the 21th ACM SIGKDD International Conference on
               Knowledge Discovery and Data Mining",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  aug,
  year      =  2015
}

@MISC{noauthor_undated-jx,
  howpublished = "\url{https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}",
  note         = "Accessed: 2025-3-3"
}

@MISC{Shelpuk2024-sl,
  title        = "{LLM} practitioner's guide: How multilingual Falcon, Mistral,
                  Smaug, and other {LLMs} are?",
  author       = "Shelpuk, Sergii",
  booktitle    = "Sergii Shelpuk",
  abstract     = "So, you want to fine-tune a large language model for your own
                  language. What model should you use? We compare the
                  multilingual performance of Llama-2, Mistral, Mixtral, Falcon,
                  Smaug, and mt5 LLMs | Shelpuk AI Technology Consulting",
  month        =  feb,
  year         =  2024,
  howpublished = "\url{https://www.shelpuk.com/post/llm-practitioner-s-guide-how-multilingual-falcon-mistral-smaug-and-other-llms-are}",
  note         = "Accessed: 2025-3-3",
  language     = "en"
}

@ARTICLE{Amari2018-nm,
  title     = "Dynamics of learning in {MLP}: Natural gradient and singularity
               revisited",
  author    = "Amari, Shun-Ichi and Ozeki, Tomoko and Karakida, Ryo and Yoshida,
               Yuki and Okada, Masato",
  journal   = "Neural Comput.",
  publisher = "MIT Press - Journals",
  volume    =  30,
  number    =  1,
  pages     = "1--33",
  abstract  = "The dynamics of supervised learning play a main role in deep
               learning, which takes place in the parameter space of a
               multilayer perceptron (MLP). We review the history of supervised
               stochastic gradient learning, focusing on its singular structure
               and natural gradient. The parameter space includes singular
               regions in which parameters are not identifiable. One of our
               results is a full exploration of the dynamical behaviors of
               stochastic gradient learning in an elementary singular network.
               The bad news is its pathological nature, in which part of the
               singular region becomes an attractor and another part a repulser
               at the same time, forming a Milnor attractor. A learning
               trajectory is attracted by the attractor region, staying in it
               for a long time, before it escapes the singular region through
               the repulser region. This is typical of plateau phenomena in
               learning. We demonstrate the strange topology of a singular
               region by introducing blow-down coordinates, which are useful for
               analyzing the natural gradient dynamics. We confirm that the
               natural gradient dynamics are free of critical slowdown. The
               second main result is the good news: the interactions of
               elementary singular networks eliminate the attractor part and the
               Milnor-type attractors disappear. This explains why large-scale
               networks do not suffer from serious critical slowdowns due to
               singularities. We finally show that the unit-wise natural
               gradient is effective for learning in spite of its low
               computational cost.",
  month     =  jan,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Hombaiah2021-sp,
  title         = "Dynamic language models for continuously evolving content",
  author        = "Hombaiah, Spurthi Amba and Chen, Tao and Zhang, Mingyang and
                   Bendersky, Michael and Najork, Marc",
  journal       = "arXiv [cs.CL]",
  abstract      = "The content on the web is in a constant state of flux. New
                   entities, issues, and ideas continuously emerge, while the
                   semantics of the existing conversation topics gradually
                   shift. In recent years, pre-trained language models like BERT
                   greatly improved the state-of-the-art for a large spectrum of
                   content understanding tasks. Therefore, in this paper, we aim
                   to study how these language models can be adapted to better
                   handle continuously evolving web content. In our study, we
                   first analyze the evolution of 2013 - 2019 Twitter data, and
                   unequivocally confirm that a BERT model trained on past
                   tweets would heavily deteriorate when directly applied to
                   data from later years. Then, we investigate two possible
                   sources of the deterioration: the semantic shift of existing
                   tokens and the sub-optimal or failed understanding of new
                   tokens. To this end, we both explore two different vocabulary
                   composition methods, as well as propose three sampling
                   methods which help in efficient incremental training for
                   BERT-like models. Compared to a new model trained from
                   scratch offline, our incremental training (a) reduces the
                   training costs, (b) achieves better performance on evolving
                   content, and (c) is suitable for online deployment. The
                   superiority of our methods is validated using two downstream
                   tasks. We demonstrate significant improvements when
                   incrementally evolving the model from a particular base year,
                   on the task of Country Hashtag Prediction, as well as on the
                   OffensEval 2019 task.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Tao2024-cl,
  title         = "Scaling laws with vocabulary: Larger models deserve larger
                   vocabularies",
  author        = "Tao, Chaofan and Liu, Qian and Dou, Longxu and Muennighoff,
                   Niklas and Wan, Zhongwei and Luo, Ping and Lin, Min and Wong,
                   Ngai",
  journal       = "arXiv [cs.CL]",
  abstract      = "Research on scaling large language models (LLMs) has
                   primarily focused on model parameters and training data size,
                   overlooking the role of vocabulary size. We investigate how
                   vocabulary size impacts LLM scaling laws by training models
                   ranging from 33M to 3B parameters on up to 500B characters
                   with various vocabulary configurations. We propose three
                   complementary approaches for predicting the compute-optimal
                   vocabulary size: IsoFLOPs analysis, derivative estimation,
                   and parametric fit of the loss function. Our approaches
                   converge on the conclusion that the optimal vocabulary size
                   depends on the compute budget, with larger models requiring
                   larger vocabularies. Most LLMs, however, use insufficient
                   vocabulary sizes. For example, we predict that the optimal
                   vocabulary size of Llama2-70B should have been at least 216K,
                   7 times larger than its vocabulary of 32K. We validate our
                   predictions empirically by training models with 3B parameters
                   across different FLOPs budgets. Adopting our predicted
                   optimal vocabulary size consistently improves downstream
                   performance over commonly used vocabulary sizes. By
                   increasing the vocabulary size from the conventional 32K to
                   43K, we improve performance on ARC-Challenge from 29.1 to
                   32.0 with the same 2.3e21 FLOPs. Our work highlights the
                   importance of jointly considering tokenization and model
                   scaling for efficient pre-training. The code and demo are
                   available at https://github.com/sail-sg/scaling-with-vocab
                   and https://hf.co/spaces/sail/scaling-with-vocab-demo.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Peng2023-mc,
  title         = "Instruction Tuning with {GPT}-4",
  author        = "Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley,
                   Michel and Gao, Jianfeng",
  journal       = "arXiv [cs.CL]",
  abstract      = "Prior work has shown that finetuning large language models
                   (LLMs) using machine-generated instruction-following data
                   enables such models to achieve remarkable zero-shot
                   capabilities on new tasks, and no human-written instructions
                   are needed. In this paper, we present the first attempt to
                   use GPT-4 to generate instruction-following data for LLM
                   finetuning. Our early experiments on instruction-tuned LLaMA
                   models show that the 52K English and Chinese
                   instruction-following data generated by GPT-4 leads to
                   superior zero-shot performance on new tasks to the
                   instruction-following data generated by previous
                   state-of-the-art models. We also collect feedback and
                   comparison data from GPT-4 to enable a comprehensive
                   evaluation and reward model training. We make our data
                   generated using GPT-4 as well as our codebase publicly
                   available.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Lahner2024-mw,
  title     = "On the Direct Alignment of Latent Spaces",
  author    = "L{\"{a}}hner, Zorah and Moeller, Michael",
  booktitle = "Proceedings of UniReps: the First Workshop on Unifying
               Representations in Neural Models",
  publisher = "PMLR",
  pages     = "158--169",
  abstract  = "With the wide adaption of deep learning and pre-trained models
               rises the question of how to effectively reuse existing latent
               spaces for new applications.One important question is how the
               geometry ...",
  month     =  may,
  year      =  2024,
  language  = "en"
}

@INPROCEEDINGS{Geva2022-sd,
  title     = "Transformer feed-forward layers build predictions by promoting
               concepts in the vocabulary space",
  author    = "Geva, Mor and Caciularu, Avi and Wang, Kevin and Goldberg, Yoav",
  booktitle = "Proceedings of the 2022 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "30--45",
  abstract  = "Mor Geva, Avi Caciularu, Kevin Wang, Yoav Goldberg. Proceedings
               of the 2022 Conference on Empirical Methods in Natural Language
               Processing. 2022.",
  month     =  dec,
  year      =  2022
}

@ARTICLE{Wortsman2022-bj,
  title         = "Model soups: averaging weights of multiple fine-tuned models
                   improves accuracy without increasing inference time",
  author        = "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir
                   Yitzhak and Roelofs, Rebecca and Gontijo-Lopes, Raphael and
                   Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and
                   Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig",
  journal       = "arXiv [cs.LG]",
  abstract      = "The conventional recipe for maximizing model accuracy is to
                   (1) train multiple models with various hyperparameters and
                   (2) pick the individual model which performs best on a
                   held-out validation set, discarding the remainder. In this
                   paper, we revisit the second step of this procedure in the
                   context of fine-tuning large pre-trained models, where
                   fine-tuned models often appear to lie in a single low error
                   basin. We show that averaging the weights of multiple models
                   fine-tuned with different hyperparameter configurations often
                   improves accuracy and robustness. Unlike a conventional
                   ensemble, we may average many models without incurring any
                   additional inference or memory costs -- we call the results
                   ``model soups.'' When fine-tuning large pre-trained models
                   such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup
                   recipe provides significant improvements over the best model
                   in a hyperparameter sweep on ImageNet. The resulting ViT-G
                   model, which attains 90.94\% top-1 accuracy on ImageNet,
                   achieved a new state of the art. Furthermore, we show that
                   the model soup approach extends to multiple image
                   classification and natural language processing tasks,
                   improves out-of-distribution performance, and improves
                   zero-shot performance on new downstream tasks. Finally, we
                   analytically relate the performance similarity of
                   weight-averaging and logit-ensembling to flatness of the loss
                   and confidence of the predictions, and validate this relation
                   empirically. Code is available at
                   https://github.com/mlfoundations/model-soups.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Unknown2024-lt,
  title     = "Knowledge And Capability Transfer Through Large Language Models'
               Parameters Fusing",
  booktitle = "The Thirteenth International Conference on Learning
               Representations",
  abstract  = "The post-training phase of large language models (LLMs) plays a
               pivotal role in refining models to follow instructions and align
               with human preferences. However, this phase is fraught with
               challenges, particularly in sourcing high-quality post-training
               data. This paper introduces a novel approach, termed Parameters
               Fusing, that simplifies the post-training process by amalgamating
               model parameters delta from existing instruct-tuned checkpoints
               with a new base model tailored to specific domain data obtained
               by continual pre-training. Utilizing open-weight models such as
               Meta's Llama, our method replicates the effects of the
               traditional post-training phase while significantly reducing both
               time and resource costs. Moreover, it facilitates the
               customization of model attributes (e.g., tool usage,
               instruction-following, coding proficiency, and tonal qualities)
               by adjusting parameter deltas from multiple checkpoints. This
               approach not only minimizes the challenges of post-training data
               acquisition but also provides a flexible and efficient framework
               for enhancing LLMs with domain-specific knowledge or
               capabilities.",
  month     =  oct,
  year      =  2024
}

@ARTICLE{Akiba2024-td,
  title         = "Evolutionary optimization of model merging recipes",
  author        = "Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi
                   and Ha, David",
  journal       = "arXiv [cs.NE]",
  abstract      = "We present a novel application of evolutionary algorithms to
                   automate the creation of powerful foundation models. While
                   model merging has emerged as a promising approach for LLM
                   development due to its cost-effectiveness, it currently
                   relies on human intuition and domain knowledge, limiting its
                   potential. Here, we propose an evolutionary approach that
                   overcomes this limitation by automatically discovering
                   effective combinations of diverse open-source models,
                   harnessing their collective intelligence without requiring
                   extensive additional training data or compute. Our approach
                   operates in both parameter space and data flow space,
                   allowing for optimization beyond just the weights of the
                   individual models. This approach even facilitates
                   cross-domain merging, generating models like a Japanese LLM
                   with Math reasoning capabilities. Surprisingly, our Japanese
                   Math LLM achieved state-of-the-art performance on a variety
                   of established Japanese LLM benchmarks, even surpassing
                   models with significantly more parameters, despite not being
                   explicitly trained for such tasks. Furthermore, a
                   culturally-aware Japanese VLM generated through our approach
                   demonstrates its effectiveness in describing Japanese
                   culture-specific content, outperforming previous Japanese
                   VLMs. This work not only contributes new state-of-the-art
                   models back to the open-source community, but also introduces
                   a new paradigm for automated model composition, paving the
                   way for exploring alternative, efficient approaches to
                   foundation model development.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE"
}

@ARTICLE{Deja2021-gb,
  title         = "Multiband {VAE}: Latent space alignment for knowledge
                   consolidation in continual learning",
  author        = "Deja, Kamil and Wawrzy\'{n}ski, Pawe\l{} and Masarczyk,
                   Wojciech and Marczak, Daniel and Trzci\'{n}ski, Tomasz",
  journal       = "arXiv [cs.LG]",
  abstract      = "We propose a new method for unsupervised generative continual
                   learning through realignment of Variational Autoencoder's
                   latent space. Deep generative models suffer from catastrophic
                   forgetting in the same way as other neural structures. Recent
                   generative continual learning works approach this problem and
                   try to learn from new data without forgetting previous
                   knowledge. However, those methods usually focus on artificial
                   scenarios where examples share almost no similarity between
                   subsequent portions of data - an assumption not realistic in
                   the real-life applications of continual learning. In this
                   work, we identify this limitation and posit the goal of
                   generative continual learning as a knowledge accumulation
                   task. We solve it by continuously aligning latent
                   representations of new data that we call bands in additional
                   latent space where examples are encoded independently of
                   their source task. In addition, we introduce a method for
                   controlled forgetting of past data that simplifies this
                   process. On top of the standard continual learning
                   benchmarks, we propose a novel challenging knowledge
                   consolidation scenario and show that the proposed approach
                   outperforms state-of-the-art by up to twofold across all
                   experiments and the additional real-life evaluation. To our
                   knowledge, Multiband VAE is the first method to show forward
                   and backward knowledge transfer in generative continual
                   learning.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Xiang2024-wp,
  title         = "{DKDM}: Data-Free Knowledge Distillation for Diffusion Models
                   with any architecture",
  author        = "Xiang, Qianlong and Zhang, Miao and Shang, Yuzhang and Wu,
                   Jianlong and Yan, Yan and Nie, Liqiang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Diffusion models (DMs) have demonstrated exceptional
                   generative capabilities across various areas, while they are
                   hindered by slow inference speeds and high computational
                   demands during deployment. The most common way to accelerate
                   DMs involves reducing the number of denoising steps during
                   generation, achieved through faster sampling solvers or
                   knowledge distillation (KD). In contrast to prior approaches,
                   we propose a novel method that transfers the capability of
                   large pretrained DMs to faster architectures. Specifically,
                   we employ KD in a distinct manner to compress DMs by
                   distilling their generative ability into more rapid variants.
                   Furthermore, considering that the source data is either
                   unaccessible or too enormous to store for current generative
                   models, we introduce a new paradigm for their distillation
                   without source data, termed Data-Free Knowledge Distillation
                   for Diffusion Models (DKDM). Generally, our established DKDM
                   framework comprises two main components: 1) a DKDM objective
                   that uses synthetic denoising data produced by pretrained DMs
                   to optimize faster DMs without source data, and 2) a dynamic
                   iterative distillation method that flexibly organizes the
                   synthesis of denoising data, preventing it from slowing down
                   the optimization process as the generation is slow. To our
                   knowledge, this is the first attempt at using KD to distill
                   DMs into any architecture in a data-free manner. Importantly,
                   our DKDM is orthogonal to most existing acceleration methods,
                   such as denoising step reduction, quantization and pruning.
                   Experiments show that our DKDM is capable of deriving 2x
                   faster DMs with performance remaining on par with the
                   baseline. Notably, our DKDM enables pretrained DMs to
                   function as ``datasets'' for training new DMs.",
  month         =  sep,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@INPROCEEDINGS{Liu2022-xt,
  title     = "{P}-tuning: Prompt tuning can be comparable to fine-tuning across
               scales and tasks",
  author    = "Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng and Du,
               Zhengxiao and Yang, Zhilin and Tang, Jie",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for
               Computational Linguistics (Volume 2: Short Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "61--68",
  abstract  = "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin
               Yang, Jie Tang. Proceedings of the 60th Annual Meeting of the
               Association for Computational Linguistics (Volume 2: Short
               Papers). 2022.",
  year      =  2022
}

@ARTICLE{Jin2024-vd,
  title     = "{WordTransABSA}: Enhancing Aspect-based Sentiment Analysis with
               masked language modeling for affective token prediction",
  author    = "Jin, Weiqiang and Zhao, Biao and Zhang, Yu and Huang, Jia and Yu,
               Hang",
  journal   = "Expert Syst. Appl.",
  publisher = "Elsevier BV",
  volume    =  238,
  number    =  122289,
  pages     =  122289,
  month     =  mar,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Ge2023-nf,
  title         = "Entailment as Robust Self-Learner",
  author        = "Ge, Jiaxin and Luo, Hongyin and Kim, Yoon and Glass, James",
  journal       = "arXiv [cs.CL]",
  abstract      = "Entailment has been recognized as an important metric for
                   evaluating natural language understanding (NLU) models, and
                   recent studies have found that entailment pretraining
                   benefits weakly supervised fine-tuning. In this work, we
                   design a prompting strategy that formulates a number of
                   different NLU tasks as contextual entailment. This approach
                   improves the zero-shot adaptation of pretrained entailment
                   models. Secondly, we notice that self-training
                   entailment-based models with unlabeled data can significantly
                   improve the adaptation performance on downstream tasks. To
                   achieve more stable improvement, we propose the Simple
                   Pseudo-Label Editing (SimPLE) algorithm for better
                   pseudo-labeling quality in self-training. We also found that
                   both pretrained entailment-based models and the self-trained
                   models are robust against adversarial evaluation data.
                   Experiments on binary and multi-class classification tasks
                   show that SimPLE leads to more robust self-training results,
                   indicating that the self-trained entailment models are more
                   efficient and trustworthy than large language models on
                   language understanding tasks.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Poth2023-qy,
  title         = "Adapters: A unified library for parameter-efficient and
                   modular transfer learning",
  author        = "Poth, Clifton and Sterz, Hannah and Paul, Indraneil and
                   Purkayastha, Sukannya and Engl{\"{a}}nder, Leon and Imhof,
                   Timo and Vuli\'{c}, Ivan and Ruder, Sebastian and Gurevych,
                   Iryna and Pfeiffer, Jonas",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce Adapters, an open-source library that unifies
                   parameter-efficient and modular transfer learning in large
                   language models. By integrating 10 diverse adapter methods
                   into a unified interface, Adapters offers ease of use and
                   flexible configuration. Our library allows researchers and
                   practitioners to leverage adapter modularity through
                   composition blocks, enabling the design of complex adapter
                   setups. We demonstrate the library's efficacy by evaluating
                   its performance against full fine-tuning on various NLP
                   tasks. Adapters provides a powerful tool for addressing the
                   challenges of conventional fine-tuning paradigms and
                   promoting more efficient and modular transfer learning. The
                   library is available via https://adapterhub.ml/adapters.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Bapna2019-zz,
  title     = "Simple, Scalable Adaptation for Neural Machine Translation",
  author    = "Bapna, Ankur and Firat, Orhan",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in
               Natural Language Processing and the 9th International Joint
               Conference on Natural Language Processing (EMNLP-IJCNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1538--1548",
  abstract  = "Ankur Bapna, Orhan Firat. Proceedings of the 2019 Conference on
               Empirical Methods in Natural Language Processing and the 9th
               International Joint Conference on Natural Language Processing
               (EMNLP-IJCNLP). 2019.",
  month     =  nov,
  year      =  2019
}

@INPROCEEDINGS{Conneau2020-aw,
  title     = "Unsupervised cross-lingual representation learning at scale",
  author    = "Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and
               Chaudhary, Vishrav and Wenzek, Guillaume and Guzm\'{a}n,
               Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke
               and Stoyanov, Veselin",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "8440--8451",
  abstract  = "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
               Chaudhary, Guillaume Wenzek, Francisco Guzm\'{a}n, Edouard Grave,
               Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. Proceedings of the
               58th Annual Meeting of the Association for Computational
               Linguistics. 2020.",
  year      =  2020
}

@INPROCEEDINGS{Subedi2024-ae,
  title     = "Exploring the Potential of Large Language Models ({LLMs}) for
               Low-resource Languages: A Study on Named-Entity Recognition
               ({NER}) and Part-Of-Speech ({POS}) Tagging for Nepali Language",
  author    = "Subedi, Bipesh and Regmi, Sunil and Bal, Bal Krishna and Acharya,
               Praveen",
  booktitle = "Proceedings of the 2024 Joint International Conference on
               Computational Linguistics, Language Resources and Evaluation
               (LREC-COLING 2024)",
  pages     = "6974--6979",
  abstract  = "Bipesh Subedi, Sunil Regmi, Bal Krishna Bal, Praveen Acharya.
               Proceedings of the 2024 Joint International Conference on
               Computational Linguistics, Language Resources and Evaluation
               (LREC-COLING 2024). 2024.",
  year      =  2024
}

@INPROCEEDINGS{Cahyawijaya2023-ih,
  title     = "{NusaWrites}: Constructing high-quality corpora for
               underrepresented and extremely low-resource languages",
  author    = "Cahyawijaya, Samuel and Lovenia, Holy and Koto, Fajri and
               Adhista, Dea and Dave, Emmanuel and Oktavianti, Sarah and Akbar,
               Salsabil and Lee, Jhonson and Shadieq, Nuur and Cenggoro, Tjeng
               Wawan and Linuwih, Hanung and Wilie, Bryan and Muridan, Galih and
               Winata, Genta and Moeljadi, David and Aji, Alham Fikri and
               Purwarianti, Ayu and Fung, Pascale",
  booktitle = "Proceedings of the 13th International Joint Conference on Natural
               Language Processing and the 3rd Conference of the Asia-Pacific
               Chapter of the Association for Computational Linguistics (Volume
               1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "921--945",
  abstract  = "Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Dea Adhista,
               Emmanuel Dave, Sarah Oktavianti, Salsabil Akbar, Jhonson Lee,
               Nuur Shadieq, Tjeng Wawan Cenggoro, Hanung Linuwih, Bryan Wilie,
               Galih Muridan, Genta Winata, David Moeljadi, Alham Fikri Aji, Ayu
               Purwarianti, Pascale Fung. Proceedings of the 13th International
               Joint Conference on Natural Language Processing and the 3rd
               Conference of the Asia-Pacific Chapter of the Association for
               Computational Linguistics (Volume 1: Long Papers). 2023.",
  month     =  nov,
  year      =  2023
}

@INPROCEEDINGS{Cahyawijaya2023-um,
  title     = "{InstructAlign}: High-and-low resource language alignment via
               continual crosslingual instruction tuning",
  author    = "Cahyawijaya, Samuel and Lovenia, Holy and Yu, Tiezheng and Chung,
               Willy and Fung, Pascale",
  booktitle = "Proceedings of the First Workshop in South East Asian Language
               Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "55--78",
  abstract  = "Samuel Cahyawijaya, Holy Lovenia, Tiezheng Yu, Willy Chung,
               Pascale Fung. Proceedings of the First Workshop in South East
               Asian Language Processing. 2023.",
  month     =  nov,
  year      =  2023
}

@ARTICLE{Chapman1988-dm,
  title     = "How to do Research At the {MIT} {AI} Lab",
  author    = "Chapman, David",
  publisher = "MIT Artificial Intelligence Laboratory",
  abstract  = "This document presumptuously purports to explain how to do
               research. We give heuristics that may be useful in pickup up
               specific skills needed for research (reading, writing,
               programming) and for understanding and enjoying the process
               itself (methodology, topic and advisor selection, and emotional
               factors).",
  month     =  oct,
  year      =  1988,
  keywords  = "Working Paper",
  language  = "en"
}

@INPROCEEDINGS{Senel2024-xa,
  title     = "Karde\c{s}-{NLU}: Transfer to Low-Resource Languages with the
               Help of a High-Resource Cousin -- A Benchmark and Evaluation for
               Turkic Languages",
  author    = "Senel, L{\"{u}}tfi Kerem and Ebing, Benedikt and Baghirova, Konul
               and Sch{\"{u}}tze, Hinrich and Glava\v{s}, Goran",
  booktitle = "Proceedings of the 18th Conference of the European Chapter of the
               Association for Computational Linguistics (Volume 1: Long Papers)",
  pages     = "1672--1688",
  abstract  = "L{\"{u}}tfi Kerem Senel, Benedikt Ebing, Konul Baghirova, Hinrich
               Schuetze, Goran Glava\v{s}. Proceedings of the 18th Conference of
               the European Chapter of the Association for Computational
               Linguistics (Volume 1: Long Papers). 2024.",
  year      =  2024
}

@ARTICLE{Tanwar2023-lw,
  title         = "Multilingual {LLMs} are better cross-lingual in-context
                   learners with alignment",
  author        = "Tanwar, Eshaan and Dutta, Subhabrata and Borthakur, Manish
                   and Chakraborty, Tanmoy",
  journal       = "arXiv [cs.CL]",
  abstract      = "In-context learning (ICL) unfolds as large language models
                   become capable of inferring test labels conditioned on a few
                   labeled samples without any gradient update. ICL-enabled
                   large language models provide a promising step forward toward
                   bypassing recurrent annotation costs in a low-resource
                   setting. Yet, only a handful of past studies have explored
                   ICL in a cross-lingual setting, in which the need for
                   transferring label-knowledge from a high-resource language to
                   a low-resource one is immensely crucial. To bridge the gap,
                   we provide the first in-depth analysis of ICL for
                   cross-lingual text classification. We find that the prevalent
                   mode of selecting random input-label pairs to construct the
                   prompt-context is severely limited in the case of
                   cross-lingual ICL, primarily due to the lack of alignment in
                   the input as well as the output spaces. To mitigate this, we
                   propose a novel prompt construction strategy -- Cross-lingual
                   In-context Source-Target Alignment (X-InSTA). With an
                   injected coherence in the semantics of the input examples and
                   a task-based alignment across the source and target
                   languages, X-InSTA is able to outperform random prompt
                   selection by a large margin across three different tasks
                   using 44 different cross-lingual pairs.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Han2023-et,
  title         = "Word embeddings are steers for language models",
  author        = "Han, Chi and Xu, Jialiang and Li, Manling and Fung, Yi and
                   Sun, Chenkai and Jiang, Nan and Abdelzaher, Tarek and Ji,
                   Heng",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) automatically learn word embeddings
                   during pre-training on language corpora. Although word
                   embeddings are usually interpreted as feature vectors for
                   individual words, their roles in language model generation
                   remain underexplored. In this work, we theoretically and
                   empirically revisit output word embeddings and find that
                   their linear transformations are equivalent to steering
                   language model generation styles. We name such steers
                   LM-Steers and find them existing in LMs of all sizes. It
                   requires learning parameters equal to 0.2\% of the original
                   LMs' size for steering each style. On tasks such as language
                   model detoxification and sentiment control, LM-Steers can
                   achieve comparable or superior performance compared with
                   state-of-the-art controlled generation methods while
                   maintaining a better balance with generation quality. The
                   learned LM-Steer serves as a lens in text styles: it reveals
                   that word embeddings are interpretable when associated with
                   language model generations and can highlight text spans that
                   most indicate the style differences. An LM-Steer is
                   transferrable between different language models by an
                   explicit form calculation. One can also continuously steer
                   LMs simply by scaling the LM-Steer or compose multiple
                   LM-Steers by adding their transformations. Our codes are
                   publicly available at
                   \url{https://github.com/Glaciohound/LM-Steer}.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Libovicky2020-hh,
  title     = "On the language neutrality of pre-trained multilingual
               representations",
  author    = "Libovick\'{y}, Jind\v{r}ich and Rosa, Rudolf and Fraser,
               Alexander",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP
               2020",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1663--1674",
  abstract  = "Jind\v{r}ich Libovick\'{y}, Rudolf Rosa, Alexander Fraser.
               Findings of the Association for Computational Linguistics: EMNLP
               2020. 2020.",
  month     =  nov,
  year      =  2020
}

@INPROCEEDINGS{Foroutan2022-dj,
  title     = "Discovering language-neutral sub-networks in multilingual
               language models",
  author    = "Foroutan, Negar and Banaei, Mohammadreza and Lebret, R\'{e}mi and
               Bosselut, Antoine and Aberer, Karl",
  booktitle = "Proceedings of the 2022 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "7560--7575",
  abstract  = "Negar Foroutan, Mohammadreza Banaei, R\'{e}mi Lebret, Antoine
               Bosselut, Karl Aberer. Proceedings of the 2022 Conference on
               Empirical Methods in Natural Language Processing. 2022.",
  month     =  dec,
  year      =  2022
}

@ARTICLE{Beyer2021-el,
  title         = "Knowledge distillation: A good teacher is patient and
                   consistent",
  author        = "Beyer, Lucas and Zhai, Xiaohua and Royer, Am\'{e}lie and
                   Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander",
  journal       = "arXiv [cs.CV]",
  abstract      = "There is a growing discrepancy in computer vision between
                   large-scale models that achieve state-of-the-art performance
                   and models that are affordable in practical applications. In
                   this paper we address this issue and significantly bridge the
                   gap between these two types of models. Throughout our
                   empirical investigation we do not aim to necessarily propose
                   a new method, but strive to identify a robust and effective
                   recipe for making state-of-the-art large scale models
                   affordable in practice. We demonstrate that, when performed
                   correctly, knowledge distillation can be a powerful tool for
                   reducing the size of large models without compromising their
                   performance. In particular, we uncover that there are certain
                   implicit design choices, which may drastically affect the
                   effectiveness of distillation. Our key contribution is the
                   explicit identification of these design choices, which were
                   not previously articulated in the literature. We back up our
                   findings by a comprehensive empirical study, demonstrate
                   compelling results on a wide range of vision datasets and, in
                   particular, obtain a state-of-the-art ResNet-50 model for
                   ImageNet, which achieves 82.8\% top-1 accuracy.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Raffel2019-qy,
  title         = "Exploring the limits of transfer learning with a unified
                   text-to-text transformer",
  author        = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee,
                   Katherine and Narang, Sharan and Matena, Michael and Zhou,
                   Yanqi and Li, Wei and Liu, Peter J",
  journal       = "arXiv [cs.LG]",
  abstract      = "Transfer learning, where a model is first pre-trained on a
                   data-rich task before being fine-tuned on a downstream task,
                   has emerged as a powerful technique in natural language
                   processing (NLP). The effectiveness of transfer learning has
                   given rise to a diversity of approaches, methodology, and
                   practice. In this paper, we explore the landscape of transfer
                   learning techniques for NLP by introducing a unified
                   framework that converts all text-based language problems into
                   a text-to-text format. Our systematic study compares
                   pre-training objectives, architectures, unlabeled data sets,
                   transfer approaches, and other factors on dozens of language
                   understanding tasks. By combining the insights from our
                   exploration with scale and our new ``Colossal Clean Crawled
                   Corpus'', we achieve state-of-the-art results on many
                   benchmarks covering summarization, question answering, text
                   classification, and more. To facilitate future work on
                   transfer learning for NLP, we release our data set,
                   pre-trained models, and code.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Kim2016-ua,
  title         = "Sequence-level knowledge distillation",
  author        = "Kim, Yoon and Rush, Alexander M",
  journal       = "arXiv [cs.CL]",
  abstract      = "Neural machine translation (NMT) offers a novel alternative
                   formulation of translation that is potentially simpler than
                   statistical approaches. However to reach competitive
                   performance, NMT models need to be exceedingly large. In this
                   paper we consider applying knowledge distillation approaches
                   (Bucila et al., 2006; Hinton et al., 2015) that have proven
                   successful for reducing the size of neural models in other
                   domains to the problem of NMT. We demonstrate that standard
                   knowledge distillation applied to word-level prediction can
                   be effective for NMT, and also introduce two novel
                   sequence-level versions of knowledge distillation that
                   further improve performance, and somewhat surprisingly, seem
                   to eliminate the need for beam search (even when applied on
                   the original teacher model). Our best student model runs 10
                   times faster than its state-of-the-art teacher with little
                   loss in performance. It is also significantly better than a
                   baseline model trained without knowledge distillation: by
                   4.2/1.7 BLEU with greedy decoding/beam search. Applying
                   weight pruning on top of knowledge distillation results in a
                   student model that has 13 times fewer parameters than the
                   original teacher model, with a decrease of 0.4 BLEU.",
  month         =  jun,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Dehghani2018-iv,
  title         = "Universal Transformers",
  author        = "Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and
                   Uszkoreit, Jakob and Kaiser, \L{}ukasz",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recurrent neural networks (RNNs) sequentially process data by
                   updating their state with each new data point, and have long
                   been the de facto choice for sequence modeling tasks.
                   However, their inherently sequential computation makes them
                   slow to train. Feed-forward and convolutional architectures
                   have recently been shown to achieve superior results on some
                   sequence modeling tasks such as machine translation, with the
                   added advantage that they concurrently process all inputs in
                   the sequence, leading to easy parallelization and faster
                   training times. Despite these successes, however, popular
                   feed-forward sequence models like the Transformer fail to
                   generalize in many simple tasks that recurrent models handle
                   with ease, e.g. copying strings or even simple logical
                   inference when the string or formula lengths exceed those
                   observed at training time. We propose the Universal
                   Transformer (UT), a parallel-in-time self-attentive recurrent
                   sequence model which can be cast as a generalization of the
                   Transformer model and which addresses these issues. UTs
                   combine the parallelizability and global receptive field of
                   feed-forward sequence models like the Transformer with the
                   recurrent inductive bias of RNNs. We also add a dynamic
                   per-position halting mechanism and find that it improves
                   accuracy on several tasks. In contrast to the standard
                   Transformer, under certain assumptions, UTs can be shown to
                   be Turing-complete. Our experiments show that UTs outperform
                   standard Transformers on a wide range of algorithmic and
                   language understanding tasks, including the challenging
                   LAMBADA language modeling task where UTs achieve a new state
                   of the art, and machine translation where UTs achieve a 0.9
                   BLEU improvement over Transformers on the WMT14 En-De
                   dataset.",
  month         =  jul,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Bousmalis2016-pt,
  title         = "Domain Separation Networks",
  author        = "Bousmalis, Konstantinos and Trigeorgis, George and Silberman,
                   Nathan and Krishnan, Dilip and Erhan, Dumitru",
  journal       = "arXiv [cs.CV]",
  abstract      = "The cost of large scale data collection and annotation often
                   makes the application of machine learning algorithms to new
                   tasks or datasets prohibitively expensive. One approach
                   circumventing this cost is training models on synthetic data
                   where annotations are provided automatically. Despite their
                   appeal, such models often fail to generalize from synthetic
                   to real images, necessitating domain adaptation algorithms to
                   manipulate these models before they can be successfully
                   applied. Existing approaches focus either on mapping
                   representations from one domain to the other, or on learning
                   to extract features that are invariant to the domain from
                   which they were extracted. However, by focusing only on
                   creating a mapping or shared representation between the two
                   domains, they ignore the individual characteristics of each
                   domain. We suggest that explicitly modeling what is unique to
                   each domain can improve a model's ability to extract
                   domain-invariant features. Inspired by work on private-shared
                   component analysis, we explicitly learn to extract image
                   representations that are partitioned into two subspaces: one
                   component which is private to each domain and one which is
                   shared across domains. Our model is trained not only to
                   perform the task we care about in the source domain, but also
                   to use the partitioned representation to reconstruct the
                   images from both domains. Our novel architecture results in a
                   model that outperforms the state-of-the-art on a range of
                   unsupervised domain adaptation scenarios and additionally
                   produces visualizations of the private and shared
                   representations enabling interpretation of the domain
                   adaptation process.",
  month         =  aug,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Wu2023-hh,
  title         = "{LaMini}-{LM}: A diverse herd of distilled models from
                   large-scale instructions",
  author        = "Wu, Minghao and Waheed, Abdul and Zhang, Chiyu and
                   Abdul-Mageed, Muhammad and Aji, Alham Fikri",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) with instruction fine-tuning
                   demonstrate superior generative capabilities. However, these
                   models are resource-intensive. To alleviate this issue, we
                   explore distilling knowledge from instruction-tuned LLMs into
                   much smaller ones. To this end, we carefully develop a large
                   set of 2.58M instructions based on both existing and
                   newly-generated instructions. In addition to being sizable,
                   we design our instructions to cover a broad set of topics to
                   ensure diversity. Extensive analysis of our instruction
                   dataset confirms its diversity, and we generate responses for
                   these instructions using gpt-3.5-turbo. Leveraging these
                   instructions, we fine-tune a diverse herd of models,
                   collectively referred to as LaMini-LM, which includes models
                   from both the encoder-decoder and decoder-only families, with
                   varying sizes. We evaluate the performance of our models
                   using automatic metrics on 15 different natural language
                   processing (NLP) benchmarks, as well as through human
                   assessment. The results demonstrate that our proposed
                   LaMini-LM models are comparable to competitive baselines,
                   while being much smaller in size.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Tjhi2023-fc,
  title     = "{SEA}-{LION} (southeast Asian languages in one network): A family
               of southeast Asian language models",
  author    = "Tjhi, William and Ong, David and Limkonchotiwat, Peerat",
  booktitle = "Proceedings of the 3rd Workshop for Natural Language Processing
               Open Source Software (NLP-OSS 2023)",
  publisher = "Empirical Methods in Natural Language Processing",
  address   = "Stroudsburg, PA, USA",
  pages     = "245--245",
  abstract  = "David Ong, Peerat Limkonchotiwat. Proceedings of the 3rd Workshop
               for Natural Language Processing Open Source Software (NLP-OSS
               2023). 2023.",
  month     =  dec,
  year      =  2023
}

@ARTICLE{Iliopoulos2022-zd,
  title         = "Weighted distillation with unlabeled examples",
  author        = "Iliopoulos, Fotis and Kontonis, Vasilis and Baykal, Cenk and
                   Menghani, Gaurav and Trinh, Khoa and Vee, Erik",
  journal       = "arXiv [cs.LG]",
  abstract      = "Distillation with unlabeled examples is a popular and
                   powerful method for training deep neural networks in settings
                   where the amount of labeled data is limited: A large
                   ''teacher'' neural network is trained on the labeled data
                   available, and then it is used to generate labels on an
                   unlabeled dataset (typically much larger in size). These
                   labels are then utilized to train the smaller ''student''
                   model which will actually be deployed. Naturally, the success
                   of the approach depends on the quality of the teacher's
                   labels, since the student could be confused if trained on
                   inaccurate data. This paper proposes a principled approach
                   for addressing this issue based on a ''debiasing''
                   reweighting of the student's loss function tailored to the
                   distillation training paradigm. Our method is hyper-parameter
                   free, data-agnostic, and simple to implement. We demonstrate
                   significant improvements on popular academic datasets and we
                   accompany our results with a theoretical analysis which
                   rigorously justifies the performance of our method in certain
                   settings.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Leong2023-ce,
  title         = "{BHASA}: A holistic Southeast Asian linguistic and cultural
                   evaluation suite for Large Language Models",
  author        = "Leong, Wei Qi and Ngui, Jian Gang and Susanto, Yosephine and
                   Rengarajan, Hamsawardhini and Sarveswaran, Kengatharaiyer and
                   Tjhi, William Chandra",
  journal       = "arXiv [cs.CL]",
  abstract      = "The rapid development of Large Language Models (LLMs) and the
                   emergence of novel abilities with scale have necessitated the
                   construction of holistic, diverse and challenging benchmarks
                   such as HELM and BIG-bench. However, at the moment, most of
                   these benchmarks focus only on performance in English and
                   evaluations that include Southeast Asian (SEA) languages are
                   few in number. We therefore propose BHASA, a holistic
                   linguistic and cultural evaluation suite for LLMs in SEA
                   languages. It comprises three components: (1) a NLP benchmark
                   covering eight tasks across Natural Language Understanding
                   (NLU), Generation (NLG) and Reasoning (NLR) tasks, (2)
                   LINDSEA, a linguistic diagnostic toolkit that spans the gamut
                   of linguistic phenomena including syntax, semantics and
                   pragmatics, and (3) a cultural diagnostics dataset that
                   probes for both cultural representation and sensitivity. For
                   this preliminary effort, we implement the NLP benchmark only
                   for Indonesian, Vietnamese, Thai and Tamil, and we only
                   include Indonesian and Tamil for LINDSEA and the cultural
                   diagnostics dataset. As GPT-4 is purportedly one of the
                   best-performing multilingual LLMs at the moment, we use it as
                   a yardstick to gauge the capabilities of LLMs in the context
                   of SEA languages. Our initial experiments on GPT-4 with BHASA
                   find it lacking in various aspects of linguistic
                   capabilities, cultural representation and sensitivity in the
                   targeted SEA languages. BHASA is a work in progress and will
                   continue to be improved and expanded in the future. The
                   repository for this paper can be found at:
                   https://github.com/aisingapore/BHASA",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Gupta2023-ii,
  title         = "Continual Pre-Training of Large Language Models: How to
                   (re)warm your model?",
  author        = "Gupta, Kshitij and Th\'{e}rien, Benjamin and Ibrahim, Adam
                   and Richter, Mats L and Anthony, Quentin and Belilovsky,
                   Eugene and Rish, Irina and Lesort, Timoth\'{e}e",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) are routinely pre-trained on
                   billions of tokens, only to restart the process over again
                   once new data becomes available. A much cheaper and more
                   efficient solution would be to enable the continual
                   pre-training of these models, i.e. updating pre-trained
                   models with new data instead of re-training them from
                   scratch. However, the distribution shift induced by novel
                   data typically results in degraded performance on past data.
                   Taking a step towards efficient continual pre-training, in
                   this work, we examine the effect of different warm-up
                   strategies. Our hypothesis is that the learning rate must be
                   re-increased to improve compute efficiency when training on a
                   new dataset. We study the warmup phase of models pre-trained
                   on the Pile (upstream data, 300B tokens) as we continue to
                   pre-train on SlimPajama (downstream data, 297B tokens),
                   following a linear warmup and cosine decay schedule. We
                   conduct all experiments on the Pythia 410M language model
                   architecture and evaluate performance through validation
                   perplexity. We experiment with different pre-training
                   checkpoints, various maximum learning rates, and various
                   warmup lengths. Our results show that while rewarming models
                   first increases the loss on upstream and downstream data, in
                   the longer run it improves the downstream performance,
                   outperforming models trained from
                   scratch$\unicode{x2013}$even for a large downstream dataset.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Wang2024-yt,
  title     = "Continued Pre-training on Sentence Analogies for Translation with
               Small Data",
  author    = "Wang, Liyan and Wang, Haotong and Lepage, Yves",
  booktitle = "Proceedings of the 2024 Joint International Conference on
               Computational Linguistics, Language Resources and Evaluation
               (LREC-COLING 2024)",
  pages     = "3890--3896",
  abstract  = "Liyan Wang, Haotong Wang, Yves Lepage. Proceedings of the 2024
               Joint International Conference on Computational Linguistics,
               Language Resources and Evaluation (LREC-COLING 2024). 2024.",
  year      =  2024
}

@INPROCEEDINGS{Wu2022-rz,
  title     = "Continued pretraining for better zero- and few-shot promptability",
  author    = "Wu, Zhaofeng and Logan, IV, Robert L and Walsh, Pete and Bhagia,
               Akshita and Groeneveld, Dirk and Singh, Sameer and Beltagy, Iz",
  booktitle = "Proceedings of the 2022 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "4517--4531",
  abstract  = "Zhaofeng Wu, Robert L Logan IV, Pete Walsh, Akshita Bhagia, Dirk
               Groeneveld, Sameer Singh, Iz Beltagy. Proceedings of the 2022
               Conference on Empirical Methods in Natural Language Processing.
               2022.",
  month     =  dec,
  year      =  2022
}

@ARTICLE{Yen-Ting2023-rn,
  title         = "Taiwan {LLM}: Bridging the linguistic divide with a
                   culturally aligned language model",
  author        = "Yen-Ting, Lin and Yun-Nung, Chen",
  journal       = "arXiv [cs.CL]",
  abstract      = "In the realm of language models, the nuanced linguistic and
                   cultural intricacies of Traditional Chinese, as spoken in
                   Taiwan, have been largely overlooked. This paper introduces
                   Taiwan LLM, a pioneering Large Language Model that
                   specifically caters to the Traditional Chinese language, with
                   a focus on the variant used in Taiwan. Leveraging a
                   comprehensive pretraining corpus and instruction-finetuning
                   datasets, we have developed a model that not only understands
                   the complexities of Traditional Chinese but also embodies the
                   cultural context of Taiwan. Taiwan LLM represents the first
                   of its kind, a model that is not only linguistically accurate
                   but also culturally resonant with its user base. Our
                   evaluations demonstrate that Taiwan LLM achieves superior
                   performance in understanding and generating Traditional
                   Chinese text, outperforming existing models that are
                   predominantly trained on Simplified Chinese or English. The
                   open-source release of Taiwan LLM invites collaboration and
                   further innovation, ensuring that the linguistic diversity of
                   Chinese speakers is embraced and well-served. The model,
                   datasets, and further resources are made publicly available
                   to foster ongoing research and development in this field.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Chen2018-dy,
  title     = "Adversarial deep averaging networks for cross-lingual sentiment
               classification",
  author    = "Chen, Xilun and Sun, Yu and Athiwaratkun, Ben and Cardie, Claire
               and Weinberger, Kilian",
  journal   = "Trans. Assoc. Comput. Linguist.",
  publisher = "MIT Press - Journals",
  volume    =  6,
  pages     = "557--570",
  abstract  = "In recent years great success has been achieved in sentiment
               classification for English, thanks in part to the availability of
               copious annotated resources. Unfortunately, most languages do not
               enjoy such an abundance of labeled data. To tackle the sentiment
               classification problem in low-resource languages without adequate
               annotated data, we propose an Adversarial Deep Averaging Network
               (ADAN 1 ) to transfer the knowledge learned from labeled data on
               a resource-rich source language to low-resource languages where
               only unlabeled data exist. ADAN has two discriminative branches:
               a sentiment classifier and an adversarial language discriminator.
               Both branches take input from a shared feature extractor to learn
               hidden representations that are simultaneously indicative for the
               classification task and invariant across languages. Experiments
               on Chinese and Arabic sentiment classification demonstrate that
               ADAN significantly outperforms state-of-the-art systems.",
  month     =  dec,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Cai2023-ii,
  title         = "Do large language models resemble humans in language use?",
  author        = "Cai, Zhenguang G and Duan, Xufeng and Haslett, David A and
                   Wang, Shuqi and Pickering, Martin J",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) such as ChatGPT and Vicuna have
                   shown remarkable capacities in comprehending and producing
                   language. However, their internal workings remain a black
                   box, and it is unclear whether LLMs and chatbots can develop
                   humanlike characteristics in language use. Cognitive
                   scientists have devised many experiments that probe, and have
                   made great progress in explaining, how people comprehend and
                   produce language. We subjected ChatGPT and Vicuna to 12 of
                   these experiments ranging from sounds to dialogue,
                   preregistered and with 1000 runs (i.e., iterations) per
                   experiment. ChatGPT and Vicuna replicated the human pattern
                   of language use in 10 and 7 out of the 12 experiments,
                   respectively. The models associated unfamiliar words with
                   different meanings depending on their forms, continued to
                   access recently encountered meanings of ambiguous words,
                   reused recent sentence structures, attributed causality as a
                   function of verb semantics, and accessed different meanings
                   and retrieved different words depending on an interlocutor's
                   identity. In addition, ChatGPT, but not Vicuna, nonliterally
                   interpreted implausible sentences that were likely to have
                   been corrupted by noise, drew reasonable inferences, and
                   overlooked semantic fallacies in a sentence. Finally, unlike
                   humans, neither model preferred using shorter words to convey
                   less informative content, nor did they use context to resolve
                   syntactic ambiguities. We discuss how these convergences and
                   divergences may result from the transformer architecture.
                   Overall, these experiments demonstrate that LLMs such as
                   ChatGPT (and Vicuna to a lesser extent) are humanlike in many
                   aspects of human language processing.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Waheed2024-ha,
  title     = "To distill or not to distill? On the robustness of robust
               knowledge distillation",
  author    = "Waheed, Abdul and Kadaoui, Karima and Abdul-Mageed, Muhammad",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "12603--12621",
  abstract  = "Abdul Waheed, Karima Kadaoui, Muhammad Abdul-Mageed. Proceedings
               of the 62nd Annual Meeting of the Association for Computational
               Linguistics (Volume 1: Long Papers). 2024.",
  year      =  2024
}

@INPROCEEDINGS{Pham2023-vp,
  title     = "Select, prompt, filter: Distilling large language models for
               summarizing conversations",
  author    = "Pham, Minh-Quang and Indurthi, Sathish and Chollampatt, Shamil
               and Turchi, Marco",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "12257--12265",
  abstract  = "Minh-Quang Pham, Sathish Indurthi, Shamil Chollampatt, Marco
               Turchi. Proceedings of the 2023 Conference on Empirical Methods
               in Natural Language Processing. 2023.",
  month     =  dec,
  year      =  2023
}

@INPROCEEDINGS{Borges2024-wp,
  title     = "Let me teach you: Pedagogical foundations of feedback for
               language models",
  author    = "Borges, Beatriz and Tandon, Niket and K{\"{a}}ser, Tanja and
               Bosselut, Antoine",
  booktitle = "Proceedings of the 2024 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "12082--12104",
  abstract  = "Beatriz Borges, Niket Tandon, Tanja K{\"{a}}ser, Antoine
               Bosselut. Proceedings of the 2024 Conference on Empirical Methods
               in Natural Language Processing. 2024.",
  month     =  nov,
  year      =  2024
}

@INPROCEEDINGS{Rocha2019-iu,
  title     = "A comparative analysis of unsupervised language adaptation
               methods",
  author    = "Rocha, Gil and Lopes Cardoso, Henrique",
  booktitle = "Proceedings of the 2nd Workshop on Deep Learning Approaches for
               Low-Resource NLP (DeepLo 2019)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "11--21",
  abstract  = "Gil Rocha, Henrique Lopes Cardoso. Proceedings of the 2nd
               Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo
               2019). 2019.",
  month     =  nov,
  year      =  2019
}

@INPROCEEDINGS{Sonkar2024-rq,
  title     = "Pedagogical alignment of large language models",
  author    = "Sonkar, Shashank and Ni, Kangqi and Chaudhary, Sapana and
               Baraniuk, Richard",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP
               2024",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "13641--13650",
  abstract  = "Shashank Sonkar, Kangqi Ni, Sapana Chaudhary, Richard Baraniuk.
               Findings of the Association for Computational Linguistics: EMNLP
               2024. 2024.",
  month     =  nov,
  year      =  2024
}

@INPROCEEDINGS{Mohtarami2019-xe,
  title     = "Contrastive Language Adaptation for Cross-Lingual Stance
               Detection",
  author    = "Mohtarami, Mitra and Glass, James and Nakov, Preslav",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in
               Natural Language Processing and the 9th International Joint
               Conference on Natural Language Processing (EMNLP-IJCNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "4442--4452",
  abstract  = "Mitra Mohtarami, James Glass, Preslav Nakov. Proceedings of the
               2019 Conference on Empirical Methods in Natural Language
               Processing and the 9th International Joint Conference on Natural
               Language Processing (EMNLP-IJCNLP). 2019.",
  month     =  nov,
  year      =  2019
}

@INPROCEEDINGS{Wang2012-at,
  title     = "Source Language Adaptation for Resource-Poor Machine Translation",
  author    = "Wang, Pidong and Nakov, Preslav and Ng, Hwee Tou",
  booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in
               Natural Language Processing and Computational Natural Language
               Learning",
  pages     = "286--296",
  abstract  = "Pidong Wang, Preslav Nakov, Hwee Tou Ng. Proceedings of the 2012
               Joint Conference on Empirical Methods in Natural Language
               Processing and Computational Natural Language Learning. 2012.",
  year      =  2012
}

@INPROCEEDINGS{Glandorf2024-qb,
  title     = "Towards Fine-Grained Pedagogical Control over English Grammar
               Complexity in Educational Text Generation",
  author    = "Glandorf, Dominik and Meurers, Detmar",
  booktitle = "Proceedings of the 19th Workshop on Innovative Use of NLP for
               Building Educational Applications (BEA 2024)",
  pages     = "299--308",
  abstract  = "Dominik Glandorf, Detmar Meurers. Proceedings of the 19th
               Workshop on Innovative Use of NLP for Building Educational
               Applications (BEA 2024). 2024.",
  year      =  2024
}

@INPROCEEDINGS{Pfeiffer2021-ua,
  title     = "{UNKs} everywhere: Adapting multilingual language models to new
               scripts",
  author    = "Pfeiffer, Jonas and Vuli\'{c}, Ivan and Gurevych, Iryna and
               Ruder, Sebastian",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "10186--10203",
  abstract  = "Jonas Pfeiffer, Ivan Vuli\'{c}, Iryna Gurevych, Sebastian Ruder.
               Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing. 2021.",
  month     =  nov,
  year      =  2021
}

@INPROCEEDINGS{Gururangan2020-hb,
  title     = "Don't stop pretraining: Adapt language models to domains and
               tasks",
  author    = "Gururangan, Suchin and Marasovi\'{c}, Ana and Swayamdipta, Swabha
               and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "8342--8360",
  abstract  = "Suchin Gururangan, Ana Marasovi\'{c}, Swabha Swayamdipta, Kyle
               Lo, Iz Beltagy, Doug Downey, Noah A. Smith. Proceedings of the
               58th Annual Meeting of the Association for Computational
               Linguistics. 2020.",
  year      =  2020
}

@INPROCEEDINGS{Khemchandani2021-kq,
  title     = "Exploiting language relatedness for low web-resource language
               model adaptation: An indic languages study",
  author    = "Khemchandani, Yash and Mehtani, Sarvesh and Patil, Vaidehi and
               Awasthi, Abhijeet and Talukdar, Partha and Sarawagi, Sunita",
  booktitle = "Proceedings of the 59th Annual Meeting of the Association for
               Computational Linguistics and the 11th International Joint
               Conference on Natural Language Processing (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1312--1323",
  abstract  = "Yash Khemchandani, Sarvesh Mehtani, Vaidehi Patil, Abhijeet
               Awasthi, Partha Talukdar, Sunita Sarawagi. Proceedings of the
               59th Annual Meeting of the Association for Computational
               Linguistics and the 11th International Joint Conference on
               Natural Language Processing (Volume 1: Long Papers). 2021.",
  year      =  2021
}

@INPROCEEDINGS{Sharoff2017-km,
  title     = "Toward Pan-Slavic {NLP}: Some Experiments with Language
               Adaptation",
  author    = "Sharoff, Serge",
  booktitle = "Proceedings of the 6th Workshop on Balto-Slavic Natural Language
               Processing",
  pages     = "1--2",
  abstract  = "There is great variation in the amount of NLP resources available
               for Slavic languages. For example, the Universal Dependency
               treebank (Nivre et al., 2016) has about 2 MW of training
               resources for Czech, more than 1 MW for Russian, while only 950
               words for Ukrainian and nothing for Belorussian, Bosnian or
               Macedonian. Similarly, the Autodesk Machine Translation dataset
               only covers three Slavic languages (Czech, Polish and Russian).
               In this talk I present a general approach, which can be called
               Language Adaptation, similarly to Domain Adaptation. In this
               approach, a model for a particular language processing task is
               built by lexical transfer of cognate words and by learning a new
               feature representation for a lesser-resourced (recipient)
               language starting from a better-resourced (donor) language. More
               specifically, I demonstrate how language adaptation works in such
               training scenarios as Translation Quality Estimation,
               Part-of-Speech tagging and Named Entity Recognition.",
  year      =  2017
}

@ARTICLE{Carlsson2024-wf,
  title         = "The hyperfitting phenomenon: Sharpening and stabilizing
                   {LLMs} for open-ended text generation",
  author        = "Carlsson, Fredrik and Liu, Fangyu and Ward, Daniel and
                   Kurfali, Murathan and Nivre, Joakim",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper introduces the counter-intuitive generalization
                   results of overfitting pre-trained large language models
                   (LLMs) on very small datasets. In the setting of open-ended
                   text generation, it is well-documented that LLMs tend to
                   generate repetitive and dull sequences, a phenomenon that is
                   especially apparent when generating using greedy decoding.
                   This issue persists even with state-of-the-art LLMs
                   containing billions of parameters, trained via next-token
                   prediction on large datasets. We find that by further
                   fine-tuning these models to achieve a near-zero training loss
                   on a small set of samples -- a process we refer to as
                   hyperfitting -- the long-sequence generative capabilities are
                   greatly enhanced. Greedy decoding with these Hyperfitted
                   models even outperform Top-P sampling over long-sequences,
                   both in terms of diversity and human preferences. This
                   phenomenon extends to LLMs of various sizes, different
                   domains, and even autoregressive image generation. We further
                   find this phenomena to be distinctly different from that of
                   Grokking and double descent. Surprisingly, our experiments
                   indicate that hyperfitted models rarely fall into repeating
                   sequences they were trained on, and even explicitly blocking
                   these sequences results in high-quality output. All
                   hyperfitted models produce extremely low-entropy predictions,
                   often allocating nearly all probability to a single token.",
  month         =  dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Zhang2016-wl,
  title         = "Understanding deep learning requires rethinking
                   generalization",
  author        = "Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht,
                   Benjamin and Vinyals, Oriol",
  journal       = "arXiv [cs.LG]",
  abstract      = "Despite their massive size, successful deep artificial neural
                   networks can exhibit a remarkably small difference between
                   training and test performance. Conventional wisdom attributes
                   small generalization error either to properties of the model
                   family, or to the regularization techniques used during
                   training. Through extensive systematic experiments, we show
                   how these traditional approaches fail to explain why large
                   neural networks generalize well in practice. Specifically,
                   our experiments establish that state-of-the-art convolutional
                   networks for image classification trained with stochastic
                   gradient methods easily fit a random labeling of the training
                   data. This phenomenon is qualitatively unaffected by explicit
                   regularization, and occurs even if we replace the true images
                   by completely unstructured random noise. We corroborate these
                   experimental findings with a theoretical construction showing
                   that simple depth two neural networks already have perfect
                   finite sample expressivity as soon as the number of
                   parameters exceeds the number of data points as it usually
                   does in practice. We interpret our experimental findings by
                   comparison with traditional models.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Zhang2021-jl,
  title     = "Understanding deep learning (still) requires rethinking
               generalization",
  author    = "Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht,
               Benjamin and Vinyals, Oriol",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  64,
  number    =  3,
  pages     = "107--115",
  abstract  = "Despite their massive size, successful deep artificial neural
               networks can exhibit a remarkably small gap between training and
               test performance. Conventional wisdom attributes small
               generalization error either to properties of the model family or
               to the regularization techniques used during training. Through
               extensive systematic experiments, we show how these traditional
               approaches fail to explain why large neural networks generalize
               well in practice. Specifically, our experiments establish that
               state-of-the-art convolutional networks for image classification
               trained with stochastic gradient methods easily fit a random
               labeling of the training data. This phenomenon is qualitatively
               unaffected by explicit regularization and occurs even if we
               replace the true images by completely unstructured random noise.
               We corroborate these experimental findings with a theoretical
               construction showing that simple depth two neural networks
               already have perfect finite sample expressivity as soon as the
               number of parameters exceeds the number of data points as it
               usually does in practice. We interpret our experimental findings
               by comparison with traditional models. We supplement this
               republication with a new section at the end summarizing recent
               progresses in the field since the original version of this paper.",
  month     =  mar,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Power2022-ci,
  title         = "Grokking: Generalization beyond overfitting on small
                   algorithmic datasets",
  author        = "Power, Alethea and Burda, Yuri and Edwards, Harri and
                   Babuschkin, Igor and Misra, Vedant",
  journal       = "arXiv [cs.LG]",
  abstract      = "In this paper we propose to study generalization of neural
                   networks on small algorithmically generated datasets. In this
                   setting, questions about data efficiency, memorization,
                   generalization, and speed of learning can be studied in great
                   detail. In some situations we show that neural networks learn
                   through a process of ``grokking'' a pattern in the data,
                   improving generalization performance from random chance level
                   to perfect generalization, and that this improvement in
                   generalization can happen well past the point of overfitting.
                   We also study generalization as a function of dataset size
                   and find that smaller datasets require increasing amounts of
                   optimization for generalization. We argue that these datasets
                   provide a fertile ground for studying a poorly understood
                   aspect of deep learning: generalization of overparametrized
                   neural networks beyond memorization of the finite training
                   dataset.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Liu2022-my,
  title         = "Omnigrok: Grokking beyond algorithmic data",
  author        = "Liu, Ziming and Michaud, Eric J and Tegmark, Max",
  journal       = "arXiv [cs.LG]",
  abstract      = "Grokking, the unusual phenomenon for algorithmic datasets
                   where generalization happens long after overfitting the
                   training data, has remained elusive. We aim to understand
                   grokking by analyzing the loss landscapes of neural networks,
                   identifying the mismatch between training and test losses as
                   the cause for grokking. We refer to this as the ``LU
                   mechanism'' because training and test losses (against model
                   weight norm) typically resemble ``L'' and ``U'',
                   respectively. This simple mechanism can nicely explain many
                   aspects of grokking: data size dependence, weight decay
                   dependence, the emergence of representations, etc. Guided by
                   the intuitive picture, we are able to induce grokking on
                   tasks involving images, language and molecules. In the
                   reverse direction, we are able to eliminate grokking for
                   algorithmic datasets. We attribute the dramatic nature of
                   grokking for algorithmic datasets to representation learning.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Nakkiran2019-as,
  title         = "Deep double descent: Where bigger models and more data hurt",
  author        = "Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and
                   Yang, Tristan and Barak, Boaz and Sutskever, Ilya",
  journal       = "arXiv [cs.LG]",
  abstract      = "We show that a variety of modern deep learning tasks exhibit
                   a ``double-descent'' phenomenon where, as we increase model
                   size, performance first gets worse and then gets better.
                   Moreover, we show that double descent occurs not just as a
                   function of model size, but also as a function of the number
                   of training epochs. We unify the above phenomena by defining
                   a new complexity measure we call the effective model
                   complexity and conjecture a generalized double descent with
                   respect to this measure. Furthermore, our notion of model
                   complexity allows us to identify certain regimes where
                   increasing (even quadrupling) the number of train samples
                   actually hurts test performance.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.

@INPROCEEDINGS{Xu2017-xb,
  title     = "Cross-lingual Distillation for Text Classification",
  author    = "Xu, Ruochen and (), Yiming Yang",
  booktitle = "Proceedings of the 55th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  pages     = "1415--1425",
  abstract  = "Cross-lingual text classification(CLTC) is the task of
               classifying documents written in different languages into the
               same taxonomy of categories. This paper presents a novel approach
               to CLTC that builds on model distillation, which adapts and
               extends a framework originally proposed for model compression.
               Using soft probabilistic predictions for the documents in a
               label-rich language as the (induced) supervisory labels in a
               parallel corpus of documents, we train classifiers successfully
               for new languages in which labeled training data are not
               available. An adversarial feature adaptation technique is also
               applied during the model training to reduce distribution
               mismatch. We conducted experiments on two benchmark CLTC
               datasets, treating English as the source language and German,
               French, Japan and Chinese as the unlabeled target languages. The
               proposed approach had the advantageous or comparable performance
               of the other state-of-art methods.",
  year      =  2017
}

@ARTICLE{Meeus2024-bw,
  title         = "{ChocoLlama}: Lessons learned from teaching llamas Dutch",
  author        = "Meeus, Matthieu and Rath\'{e}, Anthony and Remy, Fran\c{c}ois
                   and Delobelle, Pieter and Decorte, Jens-Joris and Demeester,
                   Thomas",
  journal       = "arXiv [cs.CL]",
  abstract      = "While Large Language Models (LLMs) have shown remarkable
                   capabilities in natural language understanding and
                   generation, their performance often lags in lower-resource,
                   non-English languages due to biases in the training data. In
                   this work, we explore strategies for adapting the primarily
                   English LLMs (Llama-2 and Llama-3) to Dutch, a language
                   spoken by 30 million people worldwide yet often
                   underrepresented in LLM development. We collect 104GB of
                   Dutch text ($32$B tokens) from various sources to first apply
                   continued pretraining using low-rank adaptation (LoRA),
                   complemented with Dutch posttraining strategies provided by
                   prior work. For Llama-2, we consider using (i) the tokenizer
                   of the original model, and (ii) training a new,
                   Dutch-specific tokenizer combined with embedding
                   reinitialization. We evaluate our adapted models,
                   ChocoLlama-2, both on standard benchmarks and a novel Dutch
                   benchmark, ChocoLlama-Bench. Our results demonstrate that
                   LoRA can effectively scale for language adaptation, and that
                   tokenizer modification with careful weight reinitialization
                   can improve performance. Notably, Llama-3 was released during
                   the course of this project and, upon evaluation, demonstrated
                   superior Dutch capabilities compared to our Dutch-adapted
                   versions of Llama-2. We hence apply the same adaptation
                   technique to Llama-3, using its original tokenizer. While our
                   adaptation methods enhanced Llama-2's Dutch capabilities, we
                   found limited gains when applying the same techniques to
                   Llama-3. This suggests that for ever improving, multilingual
                   foundation models, language adaptation techniques may benefit
                   more from focusing on language-specific posttraining rather
                   than on continued pretraining. We hope this work contributes
                   to the broader understanding of adapting LLMs to
                   lower-resource languages, and to the development of Dutch
                   LLMs in particular.",
  month         =  dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Tri2024-ke,
  title         = "Transformers are {SSMs}: Generalized models and efficient
                   algorithms through structured state space duality",
  author        = "Tri, Dao and Albert, Gu",
  journal       = "arXiv [cs.LG]",
  abstract      = "While Transformers have been the main architecture behind
                   deep learning's success in language modeling, state-space
                   models (SSMs) such as Mamba have recently been shown to match
                   or outperform Transformers at small to medium scale. We show
                   that these families of models are actually quite closely
                   related, and develop a rich framework of theoretical
                   connections between SSMs and variants of attention, connected
                   through various decompositions of a well-studied class of
                   structured semiseparable matrices. Our state space duality
                   (SSD) framework allows us to design a new architecture
                   (Mamba-2) whose core layer is an a refinement of Mamba's
                   selective SSM that is 2-8X faster, while continuing to be
                   competitive with Transformers on language modeling.",
  month         =  may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Franz2022-qz,
  title         = "An algorithm for routing vectors in sequences",
  author        = "Franz, A Heinsen",
  journal       = "arXiv [cs.LG]",
  abstract      = "We propose a routing algorithm that takes a sequence of
                   vectors and computes a new sequence with specified length and
                   vector size. Each output vector maximizes ``bang per bit,''
                   the difference between a net benefit to use and net cost to
                   ignore data, by better predicting the input vectors. We
                   describe output vectors as geometric objects, as latent
                   variables that assign credit, as query states in a model of
                   associative memory, and as agents in a model of a Society of
                   Mind. We implement the algorithm with optimizations that
                   reduce parameter count, computation, and memory use by orders
                   of magnitude, enabling us to route sequences of greater
                   length than previously possible. We evaluate our
                   implementation on natural language and visual classification
                   tasks, obtaining competitive or state-of-the-art accuracy and
                   end-to-end credit assignments that are interpretable.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Zhipeng2024-xz,
  title         = "Extracting and transferring abilities for building
                   multi-lingual ability-enhanced large language models",
  author        = "Zhipeng, Chen and Liang, Song and Kun, Zhou and Zhao, Wayne
                   Xin and Bingning, Wang and Weipeng, Chen and Ji-Rong, Wen",
  journal       = "arXiv [cs.CL]",
  abstract      = "Multi-lingual ability transfer has become increasingly
                   important for the broad application of large language models
                   (LLMs). Existing work highly relies on training with the
                   multi-lingual ability-related data, which may be not
                   available for low-resource languages. To solve it, we propose
                   a Multi-lingual Ability Extraction and Transfer approach,
                   named as MAET. Our key idea is to decompose and extract
                   language-agnostic ability-related weights from LLMs, and
                   transfer them across different languages by simple addition
                   and subtraction operations without training. Specially, our
                   MAET consists of the extraction and transfer stages. In the
                   extraction stage, we firstly locate key neurons that are
                   highly related to specific abilities, and then employ them to
                   extract the transferable ability-specific weights. In the
                   transfer stage, we further select the ability-related
                   parameter tensors, and design the merging strategy based on
                   the linguistic and ability specific weights, to build the
                   multi-lingual ability-enhanced LLM. To demonstrate the
                   effectiveness of our proposed approach, we conduct extensive
                   experiments on mathematical and scientific tasks in both
                   high-resource lingual and low-resource lingual scenarios.
                   Experiment results have shown that MAET can effectively and
                   efficiently extract and transfer the advanced abilities, and
                   outperform training-based baseline methods. Our code and data
                   are available at \url{https://github.com/RUCAIBox/MAET}.",
  month         =  oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Yuxian2023-lk,
  title         = "{MiniLLM}: Knowledge Distillation of large language models",
  author        = "Yuxian, Gu and Li, Dong and Furu, Wei and Minlie, Huang",
  journal       = "arXiv [cs.CL]",
  abstract      = "Knowledge Distillation (KD) is a promising technique for
                   reducing the high computational demand of large language
                   models (LLMs). However, previous KD methods are primarily
                   applied to white-box classification models or training small
                   models to imitate black-box model APIs like ChatGPT. How to
                   effectively distill the knowledge of white-box LLMs into
                   small models is still under-explored, which becomes more
                   important with the prosperity of open-source LLMs. In this
                   work, we propose a KD approach that distills LLMs into
                   smaller language models. We first replace the forward
                   Kullback-Leibler divergence (KLD) objective in the standard
                   KD approaches with reverse KLD, which is more suitable for KD
                   on generative language models, to prevent the student model
                   from overestimating the low-probability regions of the
                   teacher distribution. Then, we derive an effective
                   optimization approach to learn this objective. The student
                   models are named MiniLLM. Extensive experiments in the
                   instruction-following setting show that MiniLLM generates
                   more precise responses with higher overall quality, lower
                   exposure bias, better calibration, and higher long-text
                   generation performance than the baselines. Our method is
                   scalable for different model families with 120M to 13B
                   parameters. Our code, data, and model checkpoints can be
                   found in
                   https://github.com/microsoft/LMOps/tree/main/minillm.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Xiaohan2024-me,
  title         = "A survey on Knowledge Distillation of Large Language Models",
  author        = "Xiaohan, Xu and Ming, Li and Chongyang, Tao and Tao, Shen and
                   Reynold, Cheng and Jinyang, Li and Can, Xu and Dacheng, Tao
                   and Tianyi, Zhou",
  journal       = "arXiv [cs.CL]",
  abstract      = "In the era of Large Language Models (LLMs), Knowledge
                   Distillation (KD) emerges as a pivotal methodology for
                   transferring advanced capabilities from leading proprietary
                   LLMs, such as GPT-4, to their open-source counterparts like
                   LLaMA and Mistral. Additionally, as open-source LLMs
                   flourish, KD plays a crucial role in both compressing these
                   models, and facilitating their self-improvement by employing
                   themselves as teachers. This paper presents a comprehensive
                   survey of KD's role within the realm of LLM, highlighting its
                   critical function in imparting advanced knowledge to smaller
                   models and its utility in model compression and
                   self-improvement. Our survey is meticulously structured
                   around three foundational pillars: \textit{algorithm},
                   \textit{skill}, and \textit{verticalization} -- providing a
                   comprehensive examination of KD mechanisms, the enhancement
                   of specific cognitive abilities, and their practical
                   implications across diverse fields. Crucially, the survey
                   navigates the intricate interplay between data augmentation
                   (DA) and KD, illustrating how DA emerges as a powerful
                   paradigm within the KD framework to bolster LLMs'
                   performance. By leveraging DA to generate context-rich,
                   skill-specific training data, KD transcends traditional
                   boundaries, enabling open-source models to approximate the
                   contextual adeptness, ethical alignment, and deep semantic
                   insights characteristic of their proprietary counterparts.
                   This work aims to provide an insightful guide for researchers
                   and practitioners, offering a detailed overview of current
                   methodologies in KD and proposing future research directions.
                   Importantly, we firmly advocate for compliance with the legal
                   terms that regulate the use of LLMs, ensuring ethical and
                   lawful application of KD of LLMs. An associated Github
                   repository is available at
                   https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Yiming2023-uq,
  title         = "Efficient and effective text encoding for Chinese {LLaMA} and
                   Alpaca",
  author        = "Yiming, Cui and Ziqing, Yang and Xin, Yao",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large Language Models (LLMs), such as ChatGPT and GPT-4, have
                   dramatically transformed natural language processing research
                   and shown promising strides towards Artificial General
                   Intelligence (AGI). Nonetheless, the high costs associated
                   with training and deploying LLMs present substantial
                   obstacles to transparent, accessible academic research. While
                   several large language models, such as LLaMA, have been
                   open-sourced by the community, these predominantly focus on
                   English corpora, limiting their usefulness for other
                   languages. In this paper, we propose a method to augment
                   LLaMA with capabilities for understanding and generating
                   Chinese text and its ability to follow instructions. We
                   achieve this by extending LLaMA's existing vocabulary with an
                   additional 20,000 Chinese tokens, thereby improving its
                   encoding efficiency and semantic understanding of Chinese. We
                   further incorporate secondary pre-training using Chinese data
                   and fine-tune the model with Chinese instruction datasets,
                   significantly enhancing the model's ability to comprehend and
                   execute instructions. Our experimental results indicate that
                   the newly proposed model markedly enhances the original
                   LLaMA's proficiency in understanding and generating Chinese
                   content. Additionally, the results on the C-Eval dataset
                   yield competitive performance among the models with several
                   times the size of ours. We have made our pre-trained models,
                   training scripts, and other resources available through
                   GitHub, fostering open research for our community. Chinese
                   LLaMA series:
                   \url{https://github.com/ymcui/Chinese-LLaMA-Alpaca} and
                   Chinese Llama-2 series:
                   \url{https://github.com/ymcui/Chinese-LLaMA-Alpaca-2}",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Zoltan2023-wb,
  title         = "Efficiently adapting pretrained language models to new
                   languages",
  author        = "Zoltan, Csaki and Pian, Pawakapan and Urmish, Thakker and
                   Qiantong, Xu",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent large language models (LLM) exhibit sub-optimal
                   performance on low-resource languages, as the training data
                   of these models is usually dominated by English and other
                   high-resource languages. Furthermore, it is challenging to
                   train models for low-resource languages, especially from
                   scratch, due to a lack of high quality training data.
                   Adapting pretrained LLMs reduces the need for data in the new
                   language while also providing cross lingual transfer
                   capabilities. However, naively adapting to new languages
                   leads to catastrophic forgetting and poor tokenizer
                   efficiency. In this work, we study how to efficiently adapt
                   any existing pretrained LLM to a new language without running
                   into these issues. In particular, we improve the encoding
                   efficiency of the tokenizer by adding new tokens from the
                   target language and study the data mixing recipe to mitigate
                   forgetting. Our experiments on adapting an English LLM to
                   Hungarian and Thai show that our recipe can reach better
                   performance than open source models on the target language,
                   with minimal regressions on English.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Zhao2024-ll,
  title         = "{LLaMA} beyond English: An empirical study on language
                   capability transfer",
  author        = "Zhao, Jun and Zhang, Zhihao and Zhang, Qi and Gui, Tao and
                   Huang, Xuanjing",
  journal       = "arXiv [cs.CL]",
  abstract      = "In recent times, substantial advancements have been witnessed
                   in large language models (LLMs), exemplified by ChatGPT,
                   showcasing remarkable proficiency across a range of complex
                   tasks. However, many mainstream LLMs (e.g. LLaMA) are
                   pretrained on English-dominant corpus, which limits their
                   performance in other non-English languages. In this paper, we
                   focus on how to effectively transfer the capabilities of
                   language generation and following instructions to a
                   non-English language. To answer this question, we conduct an
                   extensive empirical investigation based on LLaMA,
                   accumulating over 1440 GPU hours. We analyze the impact of
                   key factors such as vocabulary extension, further
                   pretraining, and instruction tuning on transfer. To
                   accurately assess the model's level of knowledge, we employ
                   four widely used standardized testing benchmarks: C-Eval,
                   MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a
                   comprehensive evaluation of the model's response quality is
                   conducted, considering aspects such as accuracy, fluency,
                   informativeness, logical coherence, and harmlessness, based
                   on LLM-Eval, a benchmarks consisting instruction tasks from
                   17 diverse categories. Our evaluation results demonstrate
                   that comparable performance to state-of-the-art transfer
                   models can be achieved with less than 1\% of the pretraining
                   data, both in terms of knowledge alignment and response
                   quality. Furthermore, the experimental outcomes across the
                   thirteen low-resource languages also exhibit similar trends.
                   We anticipate that the conclusions revealed by the
                   experiments will aid the community in developing non-English
                   LLMs.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Lopes2017-px,
  title         = "Data-free knowledge distillation for deep neural networks",
  author        = "Lopes, Raphael Gontijo and Fenu, Stefano and Starner, Thad",
  journal       = "arXiv [cs.LG]",
  abstract      = "Recent advances in model compression have provided procedures
                   for compressing large neural networks to a fraction of their
                   original size while retaining most if not all of their
                   accuracy. However, all of these approaches rely on access to
                   the original training set, which might not always be possible
                   if the network to be compressed was trained on a very large
                   dataset, or on a dataset whose release poses privacy or
                   safety concerns as may be the case for biometrics tasks. We
                   present a method for data-free knowledge distillation, which
                   is able to compress deep neural networks trained on
                   large-scale datasets to a fraction of their size leveraging
                   only some extra metadata to be provided with a pretrained
                   model release. We also explore different kinds of metadata
                   that can be used with our method, and discuss tradeoffs
                   involved in using each of them.",
  month         =  oct,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Chen2024-es,
  title     = "Efficient Unseen Language Adaptation for Multilingual Pre-Trained
               Language Models",
  author    = "Chen, Po-Heng and Chen, Yun-Nung",
  booktitle = "Proceedings of the 2024 Conference on Empirical Methods in
               Natural Language Processing",
  pages     = "18983--18994",
  abstract  = "Po-Heng Chen, Yun-Nung Chen. Proceedings of the 2024 Conference
               on Empirical Methods in Natural Language Processing. 2024.",
  month     =  nov,
  year      =  2024
}

@ARTICLE{Huttebraucker2024-or,
  title         = "Latent space alignment for semantic channel equalization",
  author        = "H{\"{u}}ttebr{\"{a}}ucker, Tom\'{a}s and Sana, Mohamed and
                   Strinati, Emilio Calvanese",
  journal       = "arXiv [cs.LG]",
  abstract      = "We relax the constraint of a shared language between agents
                   in a semantic and goal-oriented communication system to
                   explore the effect of language mismatch in distributed task
                   solving. We propose a mathematical framework, which provides
                   a modelling and a measure of the semantic distortion
                   introduced in the communication when agents use distinct
                   languages. We then propose a new approach to semantic channel
                   equalization with proven effectiveness through numerical
                   evaluations.",
  month         =  may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Hinton2015-gs,
  title         = "Distilling the knowledge in a neural network",
  author        = "Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff",
  journal       = "arXiv [stat.ML]",
  abstract      = "A very simple way to improve the performance of almost any
                   machine learning algorithm is to train many different models
                   on the same data and then to average their predictions.
                   Unfortunately, making predictions using a whole ensemble of
                   models is cumbersome and may be too computationally expensive
                   to allow deployment to a large number of users, especially if
                   the individual models are large neural nets. Caruana and his
                   collaborators have shown that it is possible to compress the
                   knowledge in an ensemble into a single model which is much
                   easier to deploy and we develop this approach further using a
                   different compression technique. We achieve some surprising
                   results on MNIST and we show that we can significantly
                   improve the acoustic model of a heavily used commercial
                   system by distilling the knowledge in an ensemble of models
                   into a single model. We also introduce a new type of ensemble
                   composed of one or more full models and many specialist
                   models which learn to distinguish fine-grained classes that
                   the full models confuse. Unlike a mixture of experts, these
                   specialist models can be trained rapidly and in parallel.",
  month         =  mar,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML"
}

@ARTICLE{Snell2022-wb,
  title         = "Learning by distilling context",
  author        = "Snell, Charlie and Klein, Dan and Zhong, Ruiqi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models significantly benefit from context tokens,
                   such as prompts or scratchpads. They perform better when
                   prompted with informative instructions, and they acquire new
                   reasoning capabilities by generating a scratch-pad before
                   predicting the final answers. However, they do not
                   \textit{internalize} these performance gains, which disappear
                   when the context tokens are gone. Our work proposes to apply
                   context distillation so that a language model can improve
                   itself by internalizing these gains. Concretely, given a
                   synthetic unlabeled input for the target task, we condition
                   the model on ``[instructions] + [task-input]'' to predict
                   ``[scratch-pad] + [final answer]''; then we fine-tune the
                   same model to predict its own ``[final answer]'' conditioned
                   on the ``[task-input]'', without seeing the
                   ``[instructions]'' or using the ``[scratch-pad]''. We show
                   that context distillation is a general method to train
                   language models, and it can effectively internalize 3 types
                   of training signals. First, it can internalize abstract task
                   instructions and explanations, so we can iteratively update
                   the model parameters with new instructions and overwrite old
                   ones. Second, it can internalize step-by-step reasoning for
                   complex tasks (e.g., 8-digit addition), and such a newly
                   acquired capability proves to be useful for other downstream
                   tasks. Finally, it can internalize concrete training
                   examples, and it outperforms directly learning with gradient
                   descent by 9\% on the SPIDER Text-to-SQL dataset;
                   furthermore, combining context distillation operations can
                   internalize more training examples than the context window
                   size allows.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Brown2023-nk,
  title         = "Efficient Transformer Knowledge Distillation: A Performance
                   Review",
  author        = "Brown, Nathan and Williamson, Ashton and Anderson, Tahj and
                   Lawrence, Logan",
  journal       = "arXiv [cs.CL]",
  abstract      = "As pretrained transformer language models continue to achieve
                   state-of-the-art performance, the Natural Language Processing
                   community has pushed for advances in model compression and
                   efficient attention mechanisms to address high computational
                   requirements and limited input sequence length. Despite these
                   separate efforts, no investigation has been done into the
                   intersection of these two fields. In this work, we provide an
                   evaluation of model compression via knowledge distillation on
                   efficient attention transformers. We provide cost-performance
                   trade-offs for the compression of state-of-the-art efficient
                   attention architectures and the gains made in performance in
                   comparison to their full attention counterparts. Furthermore,
                   we introduce a new long-context Named Entity Recognition
                   dataset, GONERD, to train and test the performance of NER
                   models on long sequences. We find that distilled efficient
                   attention transformers can preserve a significant amount of
                   original model performance, preserving up to 98.6\% across
                   short-context tasks (GLUE, SQUAD, CoNLL-2003), up to 94.6\%
                   across long-context Question-and-Answering tasks (HotpotQA,
                   TriviaQA), and up to 98.8\% on long-context Named Entity
                   Recognition (GONERD), while decreasing inference times by up
                   to 57.8\%. We find that, for most models on most tasks,
                   performing knowledge distillation is an effective method to
                   yield high-performing efficient attention models with low
                   costs.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Meng2022-yy,
  title         = "Locating and editing factual associations in {GPT}",
  author        = "Meng, Kevin and Bau, David and Andonian, Alex and Belinkov,
                   Yonatan",
  journal       = "arXiv [cs.CL]",
  abstract      = "We analyze the storage and recall of factual associations in
                   autoregressive transformer language models, finding evidence
                   that these associations correspond to localized,
                   directly-editable computations. We first develop a causal
                   intervention for identifying neuron activations that are
                   decisive in a model's factual predictions. This reveals a
                   distinct set of steps in middle-layer feed-forward modules
                   that mediate factual predictions while processing subject
                   tokens. To test our hypothesis that these computations
                   correspond to factual association recall, we modify
                   feed-forward weights to update specific factual associations
                   using Rank-One Model Editing (ROME). We find that ROME is
                   effective on a standard zero-shot relation extraction (zsRE)
                   model-editing task, comparable to existing methods. To
                   perform a more sensitive evaluation, we also evaluate ROME on
                   a new dataset of counterfactual assertions, on which it
                   simultaneously maintains both specificity and generalization,
                   whereas other methods sacrifice one or another. Our results
                   confirm an important role for mid-layer feed-forward modules
                   in storing factual associations and suggest that direct
                   manipulation of computational mechanisms may be a feasible
                   approach for model editing. The code, dataset,
                   visualizations, and an interactive demo notebook are
                   available at https://rome.baulab.info/",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Parmar2024-mu,
  title         = "Reuse, don't retrain: A recipe for continued pretraining of
                   language models",
  author        = "Parmar, Jupinder and Satheesh, Sanjev and Patwary, Mostofa
                   and Shoeybi, Mohammad and Catanzaro, Bryan",
  journal       = "arXiv [cs.CL]",
  abstract      = "As language models have scaled both their number of
                   parameters and pretraining dataset sizes, the computational
                   cost for pretraining has become intractable except for the
                   most well-resourced teams. This increasing cost makes it ever
                   more important to be able to reuse a model after it has
                   completed pretraining; allowing for a model's abilities to
                   further improve without needing to train from scratch. In
                   this work, we detail a set of guidelines that cover how to
                   design efficacious data distributions and learning rate
                   schedules for continued pretraining of language models. When
                   applying these findings within a continued pretraining run on
                   top of a well-trained 15B parameter model, we show an
                   improvement of 9\% in average model accuracy compared to the
                   baseline of continued training on the pretraining set. The
                   resulting recipe provides a practical starting point with
                   which to begin developing language models through reuse
                   rather than retraining.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Kallini2024-mb,
  title         = "Mission: Impossible Language Models",
  author        = "Kallini, Julie and Papadimitriou, Isabel and Futrell, Richard
                   and Mahowald, Kyle and Potts, Christopher",
  journal       = "arXiv [cs.CL]",
  abstract      = "Chomsky and others have very directly claimed that large
                   language models (LLMs) are equally capable of learning
                   languages that are possible and impossible for humans to
                   learn. However, there is very little published experimental
                   evidence to support such a claim. Here, we develop a set of
                   synthetic impossible languages of differing complexity, each
                   designed by systematically altering English data with
                   unnatural word orders and grammar rules. These languages lie
                   on an impossibility continuum: at one end are languages that
                   are inherently impossible, such as random and irreversible
                   shuffles of English words, and on the other, languages that
                   may not be intuitively impossible but are often considered so
                   in linguistics, particularly those with rules based on
                   counting word positions. We report on a wide range of
                   evaluations to assess the capacity of GPT-2 small models to
                   learn these uncontroversially impossible languages, and
                   crucially, we perform these assessments at various stages
                   throughout training to compare the learning process for each
                   language. Our core finding is that GPT-2 struggles to learn
                   impossible languages when compared to English as a control,
                   challenging the core claim. More importantly, we hope our
                   approach opens up a productive line of inquiry in which
                   different LLM architectures are tested on a variety of
                   impossible languages in an effort to learn more about how
                   LLMs can be used as tools for these cognitive and typological
                   investigations.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Shoeybi2019-bx,
  title         = "Megatron-{LM}: Training multi-billion parameter language
                   models using model parallelism",
  author        = "Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and
                   LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent work in language modeling demonstrates that training
                   large transformer models advances the state of the art in
                   Natural Language Processing applications. However, very large
                   models can be quite difficult to train due to memory
                   constraints. In this work, we present our techniques for
                   training very large transformer models and implement a
                   simple, efficient intra-layer model parallel approach that
                   enables training transformer models with billions of
                   parameters. Our approach does not require a new compiler or
                   library changes, is orthogonal and complimentary to pipeline
                   model parallelism, and can be fully implemented with the
                   insertion of a few communication operations in native
                   PyTorch. We illustrate this approach by converging
                   transformer based models up to 8.3 billion parameters using
                   512 GPUs. We sustain 15.1 PetaFLOPs across the entire
                   application with 76\% scaling efficiency when compared to a
                   strong single GPU baseline that sustains 39 TeraFLOPs, which
                   is 30\% of peak FLOPs. To demonstrate that large language
                   models can further advance the state of the art (SOTA), we
                   train an 8.3 billion parameter transformer language model
                   similar to GPT-2 and a 3.9 billion parameter model similar to
                   BERT. We show that careful attention to the placement of
                   layer normalization in BERT-like models is critical to
                   achieving increased performance as the model size grows.
                   Using the GPT-2 model we achieve SOTA results on the
                   WikiText103 (10.8 compared to SOTA perplexity of 15.8) and
                   LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%)
                   datasets. Our BERT model achieves SOTA results on the RACE
                   dataset (90.9\% compared to SOTA accuracy of 89.4\%).",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Albert2023-hk,
  title         = "Mamba: Linear-time sequence modeling with selective state
                   spaces",
  author        = "Albert, Gu and Tri, Dao",
  journal       = "arXiv [cs.LG]",
  abstract      = "Foundation models, now powering most of the exciting
                   applications in deep learning, are almost universally based
                   on the Transformer architecture and its core attention
                   module. Many subquadratic-time architectures such as linear
                   attention, gated convolution and recurrent models, and
                   structured state space models (SSMs) have been developed to
                   address Transformers' computational inefficiency on long
                   sequences, but they have not performed as well as attention
                   on important modalities such as language. We identify that a
                   key weakness of such models is their inability to perform
                   content-based reasoning, and make several improvements.
                   First, simply letting the SSM parameters be functions of the
                   input addresses their weakness with discrete modalities,
                   allowing the model to selectively propagate or forget
                   information along the sequence length dimension depending on
                   the current token. Second, even though this change prevents
                   the use of efficient convolutions, we design a hardware-aware
                   parallel algorithm in recurrent mode. We integrate these
                   selective SSMs into a simplified end-to-end neural network
                   architecture without attention or even MLP blocks (Mamba).
                   Mamba enjoys fast inference (5$\times$ higher throughput than
                   Transformers) and linear scaling in sequence length, and its
                   performance improves on real data up to million-length
                   sequences. As a general sequence model backbone, Mamba
                   achieves state-of-the-art performance across several
                   modalities such as language, audio, and genomics. On language
                   modeling, our Mamba-3B model outperforms Transformers of the
                   same size and matches Transformers twice its size, both in
                   pretraining and downstream evaluation.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Jiang2023-va,
  title         = "A latent space theory for emergent abilities in large
                   language models",
  author        = "Jiang, Hui",
  journal       = "arXiv [cs.CL]",
  abstract      = "Languages are not created randomly but rather to communicate
                   information. There is a strong association between languages
                   and their underlying meanings, resulting in a sparse joint
                   distribution that is heavily peaked according to their
                   correlations. Moreover, these peak values happen to match
                   with the marginal distribution of languages due to the
                   sparsity. With the advent of LLMs trained on big data and
                   large models, we can now precisely assess the marginal
                   distribution of languages, providing a convenient means of
                   exploring the sparse structures in the joint distribution for
                   effective inferences. In this paper, we categorize languages
                   as either unambiguous or {\epsilon}-ambiguous and present
                   quantitative results to demonstrate that the emergent
                   abilities of LLMs, such as language understanding, in-context
                   learning, chain-of-thought prompting, and effective
                   instruction fine-tuning, can all be attributed to Bayesian
                   inference on the sparse joint distribution of languages.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Wang2021-yn,
  title         = "Entailment as few-shot learner",
  author        = "Wang, Sinong and Fang, Han and Khabsa, Madian and Mao, Hanzi
                   and Ma, Hao",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large pre-trained language models (LMs) have demonstrated
                   remarkable ability as few-shot learners. However, their
                   success hinges largely on scaling model parameters to a
                   degree that makes it challenging to train and serve. In this
                   paper, we propose a new approach, named as EFL, that can turn
                   small LMs into better few-shot learners. The key idea of this
                   approach is to reformulate potential NLP task into an
                   entailment one, and then fine-tune the model with as little
                   as 8 examples. We further demonstrate our proposed method can
                   be: (i) naturally combined with an unsupervised contrastive
                   learning-based data augmentation method; (ii) easily extended
                   to multilingual few-shot learning. A systematic evaluation on
                   18 standard NLP tasks demonstrates that this approach
                   improves the various existing SOTA few-shot learning methods
                   by 12\%, and yields competitive few-shot performance with 500
                   times larger models, such as GPT-3.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Su2024-pp,
  title         = "Extracting memorized training data via decomposition",
  author        = "Su, Ellen and Vellore, Anu and Chang, Amy and Mura, Raffaele
                   and Nelson, Blaine and Kassianik, Paul and Karbasi, Amin",
  journal       = "arXiv [cs.LG]",
  abstract      = "The widespread use of Large Language Models (LLMs) in society
                   creates new information security challenges for developers,
                   organizations, and end-users alike. LLMs are trained on large
                   volumes of data, and their susceptibility to reveal the exact
                   contents of the source training datasets poses security and
                   safety risks. Although current alignment procedures restrict
                   common risky behaviors, they do not completely prevent LLMs
                   from leaking data. Prior work demonstrated that LLMs may be
                   tricked into divulging training data by using
                   out-of-distribution queries or adversarial techniques. In
                   this paper, we demonstrate a simple, query-based
                   decompositional method to extract news articles from two
                   frontier LLMs. We use instruction decomposition techniques to
                   incrementally extract fragments of training data. Out of 3723
                   New York Times articles, we extract at least one verbatim
                   sentence from 73 articles, and over 20\% of verbatim
                   sentences from 6 articles. Our analysis demonstrates that
                   this method successfully induces the LLM to generate texts
                   that are reliable reproductions of news articles, meaning
                   that they likely originate from the source training dataset.
                   This method is simple, generalizable, and does not fine-tune
                   or change the production model. If replicable at scale, this
                   training data extraction methodology could expose new LLM
                   security and safety vulnerabilities, including privacy risks
                   and unauthorized data leaks. These implications require
                   careful consideration from model development to its end-use.",
  month         =  sep,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Liu2022-ik,
  title         = "Few-shot parameter-efficient fine-tuning is better and
                   cheaper than in-context learning",
  author        = "Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta,
                   Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin",
  journal       = "arXiv [cs.LG]",
  abstract      = "Few-shot in-context learning (ICL) enables pre-trained
                   language models to perform a previously-unseen task without
                   any gradient-based training by feeding a small number of
                   training examples as part of the input. ICL incurs
                   substantial computational, memory, and storage costs because
                   it involves processing all of the training examples every
                   time a prediction is made. Parameter-efficient fine-tuning
                   (PEFT) (e.g. adapter modules, prompt tuning, sparse update
                   methods, etc.) offers an alternative paradigm where a small
                   set of parameters are trained to enable a model to perform
                   the new task. In this paper, we rigorously compare few-shot
                   ICL and PEFT and demonstrate that the latter offers better
                   accuracy as well as dramatically lower computational costs.
                   Along the way, we introduce a new PEFT method called (IA)$^3$
                   that scales activations by learned vectors, attaining
                   stronger performance while only introducing a relatively tiny
                   amount of new parameters. We also propose a simple recipe
                   based on the T0 model called T-Few that can be applied to new
                   tasks without task-specific tuning or modifications. We
                   validate the effectiveness of T-Few on completely unseen
                   tasks by applying it to the RAFT benchmark, attaining
                   super-human performance for the first time and outperforming
                   the state-of-the-art by 6\% absolute. All of the code used in
                   our experiments is publicly available.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Yong2023-ys,
  title     = "{BLOOM+1}: Adding language support to {BLOOM} for zero-shot
               prompting",
  author    = "Yong, Zheng Xin and Schoelkopf, Hailey and Muennighoff, Niklas
               and Aji, Alham Fikri and Adelani, David Ifeoluwa and Almubarak,
               Khalid and Bari, M Saiful and Sutawika, Lintang and Kasai, Jungo
               and Baruwa, Ahmed and Winata, Genta and Biderman, Stella and
               Raff, Edward and Radev, Dragomir and Nikoulina, Vassilina",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "11682--11703",
  abstract  = "Zheng Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham
               Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful
               Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Winata,
               Stella Biderman, Edward Raff, Dragomir Radev, Vassilina
               Nikoulina. Proceedings of the 61st Annual Meeting of the
               Association for Computational Linguistics (Volume 1: Long
               Papers). 2023.",
  year      =  2023
}

@INPROCEEDINGS{Sharoff2018-ke,
  title     = "Language adaptation experiments via cross-lingual embeddings for
               related languages",
  author    = "Sharoff, Serge",
  booktitle = "Proceedings of the Eleventh International Conference on Language
               Resources and Evaluation (LREC 2018)",
  abstract  = "Serge Sharoff. Proceedings of the Eleventh International
               Conference on Language Resources and Evaluation (LREC 2018).
               2018.",
  year      =  2018
}

@INPROCEEDINGS{Gao2024-nh,
  title     = "{VE}-{KD}: Vocabulary-expansion knowledge-distillation for
               training smaller domain-specific language models",
  author    = "Gao, Pengju and Yamasaki, Tomohiro and Imoto, Kazunori",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP
               2024",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "15046--15059",
  abstract  = "Pengju Gao, Tomohiro Yamasaki, Kazunori Imoto. Findings of the
               Association for Computational Linguistics: EMNLP 2024. 2024.",
  month     =  nov,
  year      =  2024
}

@ARTICLE{Kim2024-dq,
  title         = "Efficient and effective vocabulary expansion towards
                   multilingual large language models",
  author        = "Kim, Seungduk and Choi, Seungtaek and Jeong, Myeongho",
  journal       = "arXiv [cs.CL]",
  abstract      = "This report introduces \texttt{EEVE-Korean-v1.0}, a Korean
                   adaptation of large language models that exhibit remarkable
                   capabilities across English and Korean text understanding.
                   Building on recent highly capable but English-centric LLMs,
                   such as SOLAR-10.7B and Phi-2, where non-English texts are
                   inefficiently processed with English-centric tokenizers, we
                   present an efficient and effective vocabulary expansion
                   (EEVE) method, which encompasses parameter freezing and
                   subword initialization. In contrast to previous efforts that
                   believe new embeddings require trillions of training tokens,
                   we show that our method can significantly boost non-English
                   proficiency within just 2 billion tokens. Surpassing most
                   instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of
                   January 2024, our model \texttt{EEVE-Korean-10.8B-v1.0} ranks
                   as the leading Korean pre-trained model in the open-source
                   community, according to Hugging Face's leaderboard. We
                   open-source our models on Huggingface to empower the open
                   research community in various languages.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{He2022-in,
  title         = "Knowledge Distillation as Efficient Pre-training: Faster
                   convergence, higher data-efficiency, and better
                   transferability",
  author        = "He, Ruifei and Sun, Shuyang and Yang, Jihan and Bai, Song and
                   Qi, Xiaojuan",
  journal       = "arXiv [cs.CV]",
  abstract      = "Large-scale pre-training has been proven to be crucial for
                   various computer vision tasks. However, with the increase of
                   pre-training data amount, model architecture amount, and the
                   private/inaccessible data, it is not very efficient or
                   possible to pre-train all the model architectures on
                   large-scale datasets. In this work, we investigate an
                   alternative strategy for pre-training, namely Knowledge
                   Distillation as Efficient Pre-training (KDEP), aiming to
                   efficiently transfer the learned feature representation from
                   existing pre-trained models to new student models for future
                   downstream tasks. We observe that existing Knowledge
                   Distillation (KD) methods are unsuitable towards pre-training
                   since they normally distill the logits that are going to be
                   discarded when transferred to downstream tasks. To resolve
                   this problem, we propose a feature-based KD method with
                   non-parametric feature dimension aligning. Notably, our
                   method performs comparably with supervised pre-training
                   counterparts in 3 downstream tasks and 9 downstream datasets
                   requiring 10x less data and 5x less pre-training time. Code
                   is available at https://github.com/CVMI-Lab/KDEP.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Chen2022-up,
  title         = "{DearKD}: Data-efficient early knowledge distillation for
                   vision transformers",
  author        = "Chen, Xianing and Cao, Qiong and Zhong, Yujie and Zhang, Jing
                   and Gao, Shenghua and Tao, Dacheng",
  journal       = "arXiv [cs.CV]",
  abstract      = "Transformers are successfully applied to computer vision due
                   to their powerful modeling capacity with self-attention.
                   However, the excellent performance of transformers heavily
                   depends on enormous training images. Thus, a data-efficient
                   transformer solution is urgently needed. In this work, we
                   propose an early knowledge distillation framework, which is
                   termed as DearKD, to improve the data efficiency required by
                   transformers. Our DearKD is a two-stage framework that first
                   distills the inductive biases from the early intermediate
                   layers of a CNN and then gives the transformer full play by
                   training without distillation. Further, our DearKD can be
                   readily applied to the extreme data-free case where no real
                   images are available. In this case, we propose a
                   boundary-preserving intra-divergence loss based on
                   DeepInversion to further close the performance gap against
                   the full-data counterpart. Extensive experiments on ImageNet,
                   partial ImageNet, data-free setting and other downstream
                   tasks prove the superiority of DearKD over its baselines and
                   state-of-the-art methods.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Touvron2020-ze,
  title         = "Training data-efficient image transformers \& distillation
                   through attention",
  author        = "Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and
                   Massa, Francisco and Sablayrolles, Alexandre and J\'{e}gou,
                   Herv\'{e}",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recently, neural networks purely based on attention were
                   shown to address image understanding tasks such as image
                   classification. However, these visual transformers are
                   pre-trained with hundreds of millions of images using an
                   expensive infrastructure, thereby limiting their adoption. In
                   this work, we produce a competitive convolution-free
                   transformer by training on Imagenet only. We train them on a
                   single computer in less than 3 days. Our reference vision
                   transformer (86M parameters) achieves top-1 accuracy of
                   83.1\% (single-crop evaluation) on ImageNet with no external
                   data. More importantly, we introduce a teacher-student
                   strategy specific to transformers. It relies on a
                   distillation token ensuring that the student learns from the
                   teacher through attention. We show the interest of this
                   token-based distillation, especially when using a convnet as
                   a teacher. This leads us to report results competitive with
                   convnets for both Imagenet (where we obtain up to 85.2\%
                   accuracy) and when transferring to other tasks. We share our
                   code and models.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@INPROCEEDINGS{Li2024-zi,
  title     = "{LLMR}: Knowledge Distillation with a Large Language
               Model-Induced Reward",
  author    = "Li, Dongheng and Hao, Yongchang and Mou, Lili",
  booktitle = "Proceedings of the 2024 Joint International Conference on
               Computational Linguistics, Language Resources and Evaluation
               (LREC-COLING 2024)",
  pages     = "10657--10664",
  abstract  = "Dongheng Li, Yongchang Hao, Lili Mou. Proceedings of the 2024
               Joint International Conference on Computational Linguistics,
               Language Resources and Evaluation (LREC-COLING 2024). 2024.",
  year      =  2024
}

@ARTICLE{Hsieh2023-ex,
  title         = "Distilling step-by-step! Outperforming larger language models
                   with less training data and smaller model sizes",
  author        = "Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and
                   Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and
                   Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas",
  journal       = "arXiv [cs.CL]",
  abstract      = "Deploying large language models (LLMs) is challenging because
                   they are memory inefficient and compute-intensive for
                   practical applications. In reaction, researchers train
                   smaller task-specific models by either finetuning with human
                   labels or distilling using LLM-generated labels. However,
                   finetuning and distillation require large amounts of training
                   data to achieve comparable performance to LLMs. We introduce
                   Distilling step-by-step, a new mechanism that (a) trains
                   smaller models that outperform LLMs, and (b) achieves so by
                   leveraging less training data needed by finetuning or
                   distillation. Our method extracts LLM rationales as
                   additional supervision for training small models within a
                   multi-task framework. We present three findings across 4 NLP
                   benchmarks: First, compared to both finetuning and
                   distillation, our mechanism achieves better performance with
                   much fewer labeled/unlabeled training examples. Second,
                   compared to few-shot prompted LLMs, we achieve better
                   performance using substantially smaller model sizes. Third,
                   we reduce both the model size and the amount of data required
                   to outperform LLMs; our finetuned 770M T5 model outperforms
                   the few-shot prompted 540B PaLM model using only 80\% of
                   available data on a benchmark, whereas standard finetuning
                   the same T5 model struggles to match even by using 100\% of
                   the dataset. We release the code at:
                   https://github.com/google-research/distilling-step-by-step .",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{West2022-lu,
  title     = "Symbolic knowledge distillation: From general language models to
               commonsense models",
  author    = "West, Peter and Bhagavatula, Chandra and Hessel, Jack and Hwang,
               Jena and Jiang, Liwei and Le Bras, Ronan and Lu, Ximing and
               Welleck, Sean and Choi, Yejin",
  booktitle = "Proceedings of the 2022 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "4602--4625",
  abstract  = "Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei
               Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi.
               Proceedings of the 2022 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies. 2022.",
  year      =  2022
}

@ARTICLE{Yoshimura2024-pb,
  title         = "{MambaPEFT}: Exploring parameter-efficient fine-tuning for
                   Mamba",
  author        = "Yoshimura, Masakazu and Hayashi, Teruaki and Maeda, Yota",
  journal       = "arXiv [cs.CL]",
  abstract      = "An ecosystem of Transformer-based models has been established
                   by building large models with extensive data.
                   Parameter-efficient fine-tuning (PEFT) is a crucial
                   technology for deploying these models to downstream tasks
                   with minimal cost while achieving effective performance.
                   Recently, Mamba, a State Space Model (SSM)-based model, has
                   attracted attention as a potential alternative to
                   Transformers. While many large-scale Mamba-based models have
                   been proposed, efficiently adapting pre-trained Mamba-based
                   models to downstream tasks remains unexplored. In this paper,
                   we conduct an exploratory analysis of PEFT methods for Mamba.
                   We investigate the effectiveness of existing PEFT methods for
                   Transformers when applied to Mamba. We also modify these
                   methods to better align with the Mamba architecture.
                   Additionally, we propose new Mamba-specific PEFT methods that
                   leverage the distinctive structure of Mamba. Our experiments
                   indicate that PEFT performs more effectively for Mamba than
                   Transformers. Lastly, we demonstrate how to effectively
                   combine multiple PEFT methods and provide a framework that
                   outperforms previous works. To ensure reproducibility, we
                   will release the code after publication.",
  month         =  nov,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Chow2024-ik,
  title         = "Inference-aware fine-tuning for best-of-{N} sampling in large
                   language models",
  author        = "Chow, Yinlam and Tennenholtz, Guy and Gur, Izzeddin and
                   Zhuang, Vincent and Dai, Bo and Thiagarajan, Sridhar and
                   Boutilier, Craig and Agarwal, Rishabh and Kumar, Aviral and
                   Faust, Aleksandra",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent studies have indicated that effectively utilizing
                   inference-time compute is crucial for attaining better
                   performance from large language models (LLMs). In this work,
                   we propose a novel inference-aware fine-tuning paradigm, in
                   which the model is fine-tuned in a manner that directly
                   optimizes the performance of the inference-time strategy. We
                   study this paradigm using the simple yet effective Best-of-N
                   (BoN) inference strategy, in which a verifier selects the
                   best out of a set of LLM-generated responses. We devise the
                   first imitation learning and reinforcement learning~(RL)
                   methods for BoN-aware fine-tuning, overcoming the
                   challenging, non-differentiable argmax operator within BoN.
                   We empirically demonstrate that our BoN-aware models
                   implicitly learn a meta-strategy that interleaves best
                   responses with more diverse responses that might be better
                   suited to a test-time input -- a process reminiscent of the
                   exploration-exploitation trade-off in RL. Our experiments
                   demonstrate the effectiveness of BoN-aware fine-tuning in
                   terms of improved performance and inference-time compute. In
                   particular, we show that our methods improve the Bo32
                   performance of Gemma 2B on Hendrycks MATH from 26.8\% to
                   30.8\%, and pass@32 from 60.0\% to 67.0\%, as well as the
                   pass@16 on HumanEval from 61.6\% to 67.1\%.",
  month         =  dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Pfeiffer2020-ni,
  title     = "{MAD}-{X}: An adapter-based framework for multi-task
               cross-lingual transfer",
  author    = "Pfeiffer, Jonas and Vuli\'{c}, Ivan and Gurevych, Iryna and
               Ruder, Sebastian",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in
               Natural Language Processing (EMNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "7654--7673",
  abstract  = "Jonas Pfeiffer, Ivan Vuli\'{c}, Iryna Gurevych, Sebastian Ruder.
               Proceedings of the 2020 Conference on Empirical Methods in
               Natural Language Processing (EMNLP). 2020.",
  month     =  nov,
  year      =  2020
}

@INPROCEEDINGS{Cahyawijaya2024-gd,
  title     = "{LLMs} are few-shot in-context low-resource language learners",
  author    = "Cahyawijaya, Samuel and Lovenia, Holy and Fung, Pascale",
  booktitle = "Proceedings of the 2024 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "405--433",
  abstract  = "Samuel Cahyawijaya, Holy Lovenia, Pascale Fung. Proceedings of
               the 2024 Conference of the North American Chapter of the
               Association for Computational Linguistics: Human Language
               Technologies (Volume 1: Long Papers). 2024.",
  year      =  2024
}

@ARTICLE{Zhao2022-ce,
  title         = "Decoupled Knowledge Distillation",
  author        = "Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and
                   Liang, Jiajun",
  journal       = "arXiv [cs.CV]",
  abstract      = "State-of-the-art distillation methods are mainly based on
                   distilling deep features from intermediate layers, while the
                   significance of logit distillation is greatly overlooked. To
                   provide a novel viewpoint to study logit distillation, we
                   reformulate the classical KD loss into two parts, i.e.,
                   target class knowledge distillation (TCKD) and non-target
                   class knowledge distillation (NCKD). We empirically
                   investigate and prove the effects of the two parts: TCKD
                   transfers knowledge concerning the ``difficulty'' of training
                   samples, while NCKD is the prominent reason why logit
                   distillation works. More importantly, we reveal that the
                   classical KD loss is a coupled formulation, which (1)
                   suppresses the effectiveness of NCKD and (2) limits the
                   flexibility to balance these two parts. To address these
                   issues, we present Decoupled Knowledge Distillation (DKD),
                   enabling TCKD and NCKD to play their roles more efficiently
                   and flexibly. Compared with complex feature-based methods,
                   our DKD achieves comparable or even better results and has
                   better training efficiency on CIFAR-100, ImageNet, and
                   MS-COCO datasets for image classification and object
                   detection tasks. This paper proves the great potential of
                   logit distillation, and we hope it will be helpful for future
                   research. The code is available at
                   https://github.com/megvii-research/mdistiller.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Jin2022-de,
  title    = "Black-box Knowledge Distillation",
  author   = "Jin, Ying and Wang, Jiaqi and Lin, Dahua",
  abstract = "Knowledge Distillation (KD) aims at distilling the knowledge from
              the large teacher model to a light-weight student model. Enhancing
              model efficiency effectively, mainstream methods often rely on the
              assumption that the teacher model is white-box (i.e., visible
              during distillation). However, this assumption does not always
              hold due to commercial, privacy, or safety concerns, which hinders
              these strong methods from being applied. Towards this dilemma, in
              this paper, we consider black-box knowledge distillation, an
              interesting yet challenging problem which aims at distilling
              teacher knowledge when merely the teacher predictions are
              accessible (i.e., the teacher model is invisible). Some early KD
              methods can be directly applied to black-box knowledge
              distillation, but the performance appears to be unsatisfactory. In
              this paper, we propose a simple yet effective approach, which
              makes better utilization of teacher predictions with prediction
              augmentation and multi-level prediction alignment. Through this
              framework, the student model learns from more diverse teacher
              predictions. Meanwhile, the prediction alignment is not only
              conducted at the instance level, but also at the batch and class
              level, through which the student model learns instance prediction,
              input correlation, and category correlation simultaneously.
              Extensive experiment results validate that our method enjoys
              consistently higher performance than previous black-box methods,
              and even reaches competitive performance with mainstream white-box
              methods. We promise to release our code and models to ensure
              reproducibility.",
  month    =  sep,
  year     =  2022
}

@INPROCEEDINGS{Winata2023-ka,
  title     = "{NusaX}: Multilingual parallel sentiment dataset for 10
               Indonesian local languages",
  author    = "Winata, Genta Indra and Aji, Alham Fikri and Cahyawijaya, Samuel
               and Mahendra, Rahmad and Koto, Fajri and Romadhony, Ade and
               Kurniawan, Kemal and Moeljadi, David and Prasojo, Radityo Eko and
               Fung, Pascale and Baldwin, Timothy and Lau, Jey Han and Sennrich,
               Rico and Ruder, Sebastian",
  booktitle = "Proceedings of the 17th Conference of the European Chapter of the
               Association for Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "815--834",
  abstract  = "Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Rahmad
               Mahendra, Fajri Koto, Ade Romadhony, Kemal Kurniawan, David
               Moeljadi, Radityo Eko Prasojo, Pascale Fung, Timothy Baldwin, Jey
               Han Lau, Rico Sennrich, Sebastian Ruder. Proceedings of the 17th
               Conference of the European Chapter of the Association for
               Computational Linguistics. 2023.",
  year      =  2023
}

@INPROCEEDINGS{Bird2024-qi,
  title     = "Must {NLP} be Extractive?",
  author    = "Bird, Steven",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "14915--14929",
  abstract  = "Steven Bird. Proceedings of the 62nd Annual Meeting of the
               Association for Computational Linguistics (Volume 1: Long
               Papers). 2024.",
  year      =  2024
}

@INPROCEEDINGS{Longpre2024-nq,
  title     = "A pretrainer's guide to training data: Measuring the effects of
               data age, domain coverage, quality, \& toxicity",
  author    = "Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee,
               Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny and
               Wei, Jason and Robinson, Kevin and Mimno, David and Ippolito,
               Daphne",
  booktitle = "Proceedings of the 2024 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "3245--3276",
  abstract  = "Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam
               Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson,
               David Mimno, Daphne Ippolito. Proceedings of the 2024 Conference
               of the North American Chapter of the Association for
               Computational Linguistics: Human Language Technologies (Volume 1:
               Long Papers). 2024.",
  year      =  2024
}

@INPROCEEDINGS{Wu2024-yg,
  title     = "Divide-or-conquer? Which part should you distill your {LLM}?",
  author    = "Wu, Zhuofeng and Bai, Richard He and Zhang, Aonan and Gu, Jiatao
               and Vydiswaran, V G Vinod and Jaitly, Navdeep and Zhang, Yizhe",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP
               2024",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "2572--2585",
  abstract  = "Zhuofeng Wu, Richard He Bai, Aonan Zhang, Jiatao Gu, V.G.Vinod
               Vydiswaran, Navdeep Jaitly, Yizhe Zhang. Findings of the
               Association for Computational Linguistics: EMNLP 2024. 2024.",
  month     =  nov,
  year      =  2024
}

@INPROCEEDINGS{Wendler2024-ar,
  title     = "Do llamas work in English? On the latent language of multilingual
               transformers",
  author    = "Wendler, Chris and Veselovsky, Veniamin and Monea, Giovanni and
               West, Robert",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "15366--15394",
  abstract  = "Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West.
               Proceedings of the 62nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers). 2024.",
  year      =  2024
}

@INPROCEEDINGS{Taguchi2024-ma,
  title     = "Language complexity and speech recognition accuracy: Orthographic
               complexity hurts, phonological complexity doesn't",
  author    = "Taguchi, Chihiro and Chiang, David",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "15493--15503",
  abstract  = "Chihiro Taguchi, David Chiang. Proceedings of the 62nd Annual
               Meeting of the Association for Computational Linguistics (Volume
               1: Long Papers). 2024.",
  year      =  2024
}

@ARTICLE{Ustun2024-wb,
  title         = "Aya model: An instruction finetuned open-access multilingual
                   language model",
  author        = "{\"{U}}st{\"{u}}n, Ahmet and Aryabumi, Viraat and Yong,
                   Zheng-Xin and Ko, Wei-Yin and D'souza, Daniel and Onilude,
                   Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi,
                   Hui-Lee and Kayid, Amr and Vargus, Freddie and Blunsom, Phil
                   and Longpre, Shayne and Muennighoff, Niklas and Fadaee,
                   Marzieh and Kreutzer, Julia and Hooker, Sara",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent breakthroughs in large language models (LLMs) have
                   centered around a handful of data-rich languages. What does
                   it take to broaden access to breakthroughs beyond first-class
                   citizen languages? Our work introduces Aya, a massively
                   multilingual generative language model that follows
                   instructions in 101 languages of which over 50\% are
                   considered as lower-resourced. Aya outperforms mT0 and BLOOMZ
                   on the majority of tasks while covering double the number of
                   languages. We introduce extensive new evaluation suites that
                   broaden the state-of-art for multilingual eval across 99
                   languages -- including discriminative and generative tasks,
                   human evaluation, and simulated win rates that cover both
                   held-out tasks and in-distribution performance. Furthermore,
                   we conduct detailed investigations on the optimal finetuning
                   mixture composition, data pruning, as well as the toxicity,
                   bias, and safety of our models. We open-source our
                   instruction datasets and our model at
                   https://hf.co/CohereForAI/aya-101",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Yao2021-ql,
  title     = "Adapt-and-distill: Developing small, fast and effective
               pretrained language models for domains",
  author    = "Yao, Yunzhi and Huang, Shaohan and Wang, Wenhui and Dong, Li and
               Wei, Furu",
  booktitle = "Findings of the Association for Computational Linguistics:
               ACL-IJCNLP 2021",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "460--470",
  abstract  = "Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, Furu Wei.
               Findings of the Association for Computational Linguistics:
               ACL-IJCNLP 2021. 2021.",
  year      =  2021
}

@INPROCEEDINGS{Ushio2021-ys,
  title     = "Distilling relation embeddings from pretrained language models",
  author    = "Ushio, Asahi and Camacho-Collados, Jose and Schockaert, Steven",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "9044--9062",
  abstract  = "Asahi Ushio, Jose Camacho-Collados, Steven Schockaert.
               Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing. 2021.",
  month     =  nov,
  year      =  2021
}

@ARTICLE{Zheng2024-nn,
  title         = "Breaking language barriers: Cross-lingual continual
                   pre-training at scale",
  author        = "Zheng, Wenzhen and Pan, Wenbo and Xu, Xu and Qin, Libo and
                   Yue, Li and Zhou, Ming",
  journal       = "arXiv [cs.CL]",
  abstract      = "In recent years, Large Language Models (LLMs) have made
                   significant strides towards Artificial General Intelligence.
                   However, training these models from scratch requires
                   substantial computational resources and vast amounts of text
                   data. In this paper, we explore an alternative approach to
                   constructing an LLM for a new language by continually
                   pretraining (CPT) from existing pretrained LLMs, instead of
                   using randomly initialized parameters. Based on parallel
                   experiments on 40 model sizes ranging from 40M to 5B
                   parameters, we find that 1) CPT converges faster and saves
                   significant resources in a scalable manner; 2) CPT adheres to
                   an extended scaling law derived from Hoffmann et al. (2022)
                   with a joint data-parameter scaling term; 3) The
                   compute-optimal data-parameter allocation for CPT markedly
                   differs based on our estimated scaling factors; 4) The
                   effectiveness of transfer at scale is influenced by training
                   duration and linguistic properties, while robust to data
                   replaying, a method that effectively mitigates catastrophic
                   forgetting in CPT. We hope our findings provide deeper
                   insights into the transferability of LLMs at scale for the
                   research community.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Chen2024-uw,
  title         = "{YourSkatingCoach}: A figure skating video benchmark for
                   fine-grained element analysis",
  author        = "Chen, Wei-Yi and Lin, Yi-Ling and Su, Yu-An and Yeh, Wei-Hsin
                   and Ku, Lun-Wei",
  journal       = "arXiv [cs.CV]",
  abstract      = "Combining sports and machine learning involves leveraging ML
                   algorithms and techniques to extract insight from
                   sports-related data such as player statistics, game footage,
                   and other relevant information. However, datasets related to
                   figure skating in the literature focus primarily on element
                   classification and are currently unavailable or exhibit only
                   limited access, which greatly raise the entry barrier to
                   developing visual sports technology for it. Moreover, when
                   using such data to help athletes improve their skills, we
                   find they are very coarse-grained: they work for learning
                   what an element is, but they are poorly suited to learning
                   whether the element is good or bad. Here we propose air time
                   detection, a novel motion analysis task, the goal of which is
                   to accurately detect the duration of the air time of a jump.
                   We present YourSkatingCoach, a large, novel figure skating
                   dataset which contains 454 videos of jump elements, the
                   detected skater skeletons in each video, along with the gold
                   labels of the start and ending frames of each jump, together
                   as a video benchmark for figure skating. In addition,
                   although this type of task is often viewed as classification,
                   we cast it as a sequential labeling problem and propose a
                   Transformer-based model to calculate the duration.
                   Experimental results show that the proposed model yields a
                   favorable results for a strong baseline. To further verify
                   the generalizability of the fine-grained labels, we apply the
                   same process to other sports as cross-sports tasks but for
                   coarse-grained task action classification. Here we fine-tune
                   the classification to demonstrate that figure skating, as it
                   contains the essential body movements, constitutes a strong
                   foundation for adaptation to other sports.",
  month         =  oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Rinartha2024-lo,
  title     = "Text and Speech Adjustable Bimodal Emotion Recognition in
               Conversational Interface",
  author    = "Rinartha, Komang and Ku, Lun-Wei",
  publisher = "IEEE",
  pages     = "96--101",
  abstract  = "Speech emotion recognition (SER) is a Machine Learning (ML) topic
               that is now receiving a lot of research attention. This can be
               attributed to its growing capacity, improvements in algorithms,
               and utilization in practical situations. Furthermore, speech
               contains not just auditory information but also lexical
               information. Both speech-based emotion recognition and text-based
               emotion recognition, called bimodal emotion recognition, can be
               highly beneficial for human interaction. Take an example,
               maintaining the communication between elder parents who are left
               behind after their children leave for education, employment, and
               family dynamics who can be greatly affected, both physically and
               emotionally, by feelings of loneliness and social isolation.
               Bimodal emotion recognition employs both text-based and
               audio-based methods to identify emotions. Each algorithm was
               trained separately and then combined using a late \ldots{}",
  month     =  aug,
  year      =  2024
}

@ARTICLE{Hsu2024-uz,
  title    = "Enhancing Perception: Refining Explanations of News Claims with
              {LLM} Conversations",
  author   = "Hsu, Yi-Li and Chen, Jui-Ning and Chiang, Yang Fan and Liu,
              Shang-Chien and Xiong, Aiping and Ku, Lun-Wei",
  pages    = "2129--2147",
  abstract = "We introduce Enhancing Perception, a framework for Large Language
              Models (LLMs) designed to streamline the time-intensive task
              typically undertaken by professional fact-checkers of crafting
              explanations for fake news. This study investigates the
              effectiveness of enhancing LLM explanations through conversational
              refinement. We compare various questioner agents, including
              state-of-the-art LLMs like GPT-4, Claude 2, PaLM 2, and 193
              American participants acting as human questioners. Based on the
              histories of these refinement conversations, we further generate
              comprehensive summary explanations. We evaluated the effectiveness
              of these initial, refined, and summary explanations across 40 news
              claims by involving 2,797 American participants, measuring their
              self-reported belief change regarding both real and fake claims
              after receiving the explanations. Our findings reveal that, in the
              context of fake news, explanations that have undergone
              conversational refinement--whether by GPT-4 or human questioners,
              who ask more diverse and detail-oriented questions--were
              significantly more effective than both the initial unrefined
              explanations and the summary explanations. Moreover, these refined
              explanations achieved a level of effectiveness comparable to that
              of expert-written explanations. The results highlight the
              potential of automatic explanation refinement by LLMs in debunking
              fake news claims.",
  month    =  jun,
  year     =  2024
}

@ARTICLE{Huang2024-zw,
  title         = "{SocialNLP} Fake-{EmoReact} 2021 challenge overview:
                   Predicting Fake tweets from their replies and {GIFs}",
  author        = "Huang, Chien-Kun and Chang, Yi-Ting and Ku, Lun-Wei and Li,
                   Cheng-Te and Shuai, Hong-Han",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper provides an overview of the Fake-EmoReact 2021
                   Challenge, held at the 9th SocialNLP Workshop, in conjunction
                   with NAACL 2021. The challenge requires predicting the
                   authenticity of tweets using reply context and augmented GIF
                   categories from EmotionGIF dataset. We offer the
                   Fake-EmoReact dataset with more than 453k as the experimental
                   materials, where every tweet is labeled with authenticity.
                   Twenty-four teams registered to participate in this
                   challenge, and 5 submitted their results successfully in the
                   evaluation phase. The best team achieves 93.9 on
                   Fake-EmoReact 2021 dataset using F1 score. In addition, we
                   show the definition of share task, data collection, and the
                   teams' performance that joined this challenge and their
                   approaches.",
  month         =  may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hsu2023-da,
  title         = "Is explanation the cure? Misinformation mitigation in the
                   short term and long term",
  author        = "Hsu, Yi-Li and Dai, Shih-Chieh and Xiong, Aiping and Ku,
                   Lun-Wei",
  journal       = "arXiv [cs.CL]",
  abstract      = "With advancements in natural language processing (NLP)
                   models, automatic explanation generation has been proposed to
                   mitigate misinformation on social media platforms in addition
                   to adding warning labels to identified fake news. While many
                   researchers have focused on generating good explanations, how
                   these explanations can really help humans combat fake news is
                   under-explored. In this study, we compare the effectiveness
                   of a warning label and the state-of-the-art counterfactual
                   explanations generated by GPT-4 in debunking misinformation.
                   In a two-wave, online human-subject study, participants (N =
                   215) were randomly assigned to a control group in which false
                   contents are shown without any intervention, a warning tag
                   group in which the false claims were labeled, or an
                   explanation group in which the false contents were
                   accompanied by GPT-4 generated explanations. Our results show
                   that both interventions significantly decrease participants'
                   self-reported belief in fake claims in an equivalent manner
                   for the short-term and long-term. We discuss the implications
                   of our findings and directions for future NLP-based
                   misinformation debunking strategies.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Dai2023-cp,
  title         = "{LLM}-in-the-loop: Leveraging large language model for
                   thematic analysis",
  author        = "Dai, Shih-Chieh and Xiong, Aiping and Ku, Lun-Wei",
  journal       = "arXiv [cs.CL]",
  abstract      = "Thematic analysis (TA) has been widely used for analyzing
                   qualitative data in many disciplines and fields. To ensure
                   reliable analysis, the same piece of data is typically
                   assigned to at least two human coders. Moreover, to produce
                   meaningful and useful analysis, human coders develop and
                   deepen their data interpretation and coding over multiple
                   iterations, making TA labor-intensive and time-consuming.
                   Recently the emerging field of large language models (LLMs)
                   research has shown that LLMs have the potential replicate
                   human-like behavior in various tasks: in particular, LLMs
                   outperform crowd workers on text-annotation tasks, suggesting
                   an opportunity to leverage LLMs on TA. We propose a human-LLM
                   collaboration framework (i.e., LLM-in-the-loop) to conduct TA
                   with in-context learning (ICL). This framework provides the
                   prompt to frame discussions with a LLM (e.g., GPT-3.5) to
                   generate the final codebook for TA. We demonstrate the
                   utility of this framework using survey datasets on the
                   aspects of the music listening experience and the usage of a
                   password manager. Results of the two case studies show that
                   the proposed framework yields similar coding quality to that
                   of human coders but reduces TA's labor and time demands.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Yeh2023-xv,
  title     = "{MAAIG} : Motion Analysis And Instruction Generation",
  author    = "Yeh, Wei-Hsin and Lin, Pei Hsin and Su, Yu-An and Cheng, Wen
               Hsiang and Ku, Lun-Wei",
  booktitle = "ACM Multimedia Asia Workshops",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--5",
  month     =  dec,
  year      =  2023,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.

@INPROCEEDINGS{Sha-Jiu-2025-dq,
  title     = "{VEEF}-Multi-{LLM}: Effective Vocabulary Expansion and Parameter
               Efficient Finetuning Towards Multilingual Large Language Models",
  author    = "(), Jiu Sha and Zhu, Mengxiao and (), Chong Feng and Shang,
               Yuming",
  booktitle = "Proceedings of the 31st International Conference on Computational
               Linguistics",
  pages     = "7963--7981",
  abstract  = "Jiu Sha, Mengxiao Zhu, Chong Feng, Yuming Shang. Proceedings of
               the 31st International Conference on Computational Linguistics.
               2025.",
  year      =  2025
}

@INPROCEEDINGS{Liu2024-yo,
  title     = "Gold panning in vocabulary: An adaptive method for vocabulary
               expansion of domain-specific {LLMs}",
  author    = "Liu, Chengyuan and Wang, Shihang and Qing, Lizhi and Kuang, Kun
               and Kang, Yangyang and Sun, Changlong and Wu, Fei",
  booktitle = "Proceedings of the 2024 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "7442--7459",
  abstract  = "Chengyuan Liu, Shihang Wang, Lizhi Qing, Kun Kuang, Yangyang
               Kang, Changlong Sun, Fei Wu. Proceedings of the 2024 Conference
               on Empirical Methods in Natural Language Processing. 2024.",
  month     =  nov,
  year      =  2024
}

@ARTICLE{DeepSeek-AI2024-ih,
  title         = "{DeepSeek}-{V3} Technical Report",
  author        = "{DeepSeek-AI} and Liu, Aixin and Feng, Bei and Xue, Bing and
                   Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao,
                   Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong
                   and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli
                   and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai,
                   Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and
                   Li, Guowei and Zhang, H and Bao, Han and Xu, Hanwei and Wang,
                   Haocheng and Zhang, Haowei and Ding, Honghui and Xin, Huajian
                   and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J L and
                   Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi
                   and Wang, Jiawei and Chen, Jin and Chen, Jingchang and Yuan,
                   Jingyang and Qiu, Junjie and Li, Junlong and Song, Junxiao
                   and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and
                   Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong
                   and Xu, Lei and Xia, Leyi and Zhao, Liang and Wang, Litong
                   and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang,
                   Mingchuan and Zhang, Minghua and Tang, Minghui and Li,
                   Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and
                   Zhang, Peng and Wang, Qiancheng and Zhu, Qihao and Chen,
                   Qinyu and Du, Qiushi and Chen, R J and Jin, R L and Ge, Ruiqi
                   and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Xu,
                   Runxin and Zhang, Ruoyu and Chen, Ruyi and Li, S S and Lu,
                   Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu,
                   Shaoqing and Ye, Shengfeng and Ye, Shengfeng and Ma, Shirong
                   and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou,
                   Shunfeng and Pan, Shuting and Wang, T and Yun, Tao and Pei,
                   Tian and Sun, Tianyu and Xiao, W L and Zeng, Wangding and
                   Zhao, Wanjia and An, Wei and Liu, Wen and Liang, Wenfeng and
                   Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Li, X Q and
                   Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong
                   and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and
                   Zhang, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun,
                   Xiaowen and Wang, Xiaoxiang and Cheng, Xin and Liu, Xin and
                   Xie, Xin and Liu, Xingchao and Yu, Xingkai and Song, Xinnan
                   and Shan, Xinxia and Zhou, Xinyi and Yang, Xinyu and Li,
                   Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, Y K and
                   Wang, Y Q and Wei, Y X and Zhu, Y X and Zhang, Yang and Xu,
                   Yanhong and Xu, Yanhong and Huang, Yanping and Li, Yao and
                   Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui
                   and Yu, Yi and Zheng, Yi and Zhang, Yichao and Shi, Yifan and
                   Xiong, Yiliang and He, Ying and Tang, Ying and Piao, Yishi
                   and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu,
                   Yiyuan and Guo, Yongqiang and Wu, Yu and Ou, Yuan and Zhu,
                   Yuchen and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He,
                   Yujia and Zha, Yukun and Xiong, Yunfan and Ma, Yunxian and
                   Yan, Yuting and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan
                   and Zhou, Yuyang and Wu, Z F and Ren, Z Z and Ren, Zehui and
                   Sha, Zhangli and Fu, Zhe and Xu, Zhean and Huang, Zhen and
                   Zhang, Zhen and Xie, Zhenda and Zhang, Zhengyan and Hao,
                   Zhewen and Gou, Zhibin and Ma, Zhicheng and Yan, Zhigang and
                   Shao, Zhihong and Xu, Zhipeng and Wu, Zhiyu and Zhang,
                   Zhongyu and Li, Zhuoshu and Gu, Zihui and Zhu, Zijia and Liu,
                   Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Gao,
                   Ziyi and Pan, Zizheng",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE)
                   language model with 671B total parameters with 37B activated
                   for each token. To achieve efficient inference and
                   cost-effective training, DeepSeek-V3 adopts Multi-head Latent
                   Attention (MLA) and DeepSeekMoE architectures, which were
                   thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3
                   pioneers an auxiliary-loss-free strategy for load balancing
                   and sets a multi-token prediction training objective for
                   stronger performance. We pre-train DeepSeek-V3 on 14.8
                   trillion diverse and high-quality tokens, followed by
                   Supervised Fine-Tuning and Reinforcement Learning stages to
                   fully harness its capabilities. Comprehensive evaluations
                   reveal that DeepSeek-V3 outperforms other open-source models
                   and achieves performance comparable to leading closed-source
                   models. Despite its excellent performance, DeepSeek-V3
                   requires only 2.788M H800 GPU hours for its full training. In
                   addition, its training process is remarkably stable.
                   Throughout the entire training process, we did not experience
                   any irrecoverable loss spikes or perform any rollbacks. The
                   model checkpoints are available at
                   https://github.com/deepseek-ai/DeepSeek-V3.",
  month         =  dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Kang2022-xx,
  title     = "{KALA}: Knowledge-augmented language model adaptation",
  author    = "Kang, Minki and Baek, Jinheon and Hwang, Sung Ju",
  booktitle = "Proceedings of the 2022 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "5144--5167",
  abstract  = "Minki Kang, Jinheon Baek, Sung Ju Hwang. Proceedings of the 2022
               Conference of the North American Chapter of the Association for
               Computational Linguistics: Human Language Technologies. 2022.",
  year      =  2022
}

@ARTICLE{Muennighoff2022-wy,
  title         = "Crosslingual Generalization through Multitask Finetuning",
  author        = "Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang
                   and Roberts, Adam and Biderman, Stella and Scao, Teven Le and
                   Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and
                   Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and
                   Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel
                   and Alyafeai, Zaid and Webson, Albert and Raff, Edward and
                   Raffel, Colin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Multitask prompted finetuning (MTF) has been shown to help
                   large language models generalize to new tasks in a zero-shot
                   setting, but so far explorations of MTF have focused on
                   English data and models. We apply MTF to the pretrained
                   multilingual BLOOM and mT5 model families to produce
                   finetuned variants called BLOOMZ and mT0. We find finetuning
                   large multilingual language models on English tasks with
                   English prompts allows for task generalization to non-English
                   languages that appear only in the pretraining corpus.
                   Finetuning on multilingual tasks with English prompts further
                   improves performance on English and non-English tasks leading
                   to various state-of-the-art zero-shot results. We also
                   investigate finetuning on multilingual tasks with prompts
                   that have been machine-translated from English to match the
                   language of each dataset. We find training on these
                   machine-translated prompts leads to better performance on
                   human-written prompts in the respective languages.
                   Surprisingly, we find models are capable of zero-shot
                   generalization to tasks in languages they have never
                   intentionally seen. We conjecture that the models are
                   learning higher-level capabilities that are both task- and
                   language-agnostic. In addition, we introduce xP3, a composite
                   of supervised datasets in 46 languages with English and
                   machine-translated prompts. Our code, datasets and models are
                   freely available at
                   https://github.com/bigscience-workshop/xmtf.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Da-Dalt2024-yb,
  title     = "{FLOR}: On the Effectiveness of Language Adaptation",
  author    = "Da Dalt, Severino and Llop, Joan and Baucells, Irene and
               P\`{a}mies, Marc and Xu, Yishi and Gonz\'{a}lez-Agirre, Aitor and
               Villegas, Marta",
  booktitle = "Proceedings of the 2024 Joint International Conference on
               Computational Linguistics, Language Resources and Evaluation
               (LREC-COLING 2024)",
  pages     = "7377--7388",
  abstract  = "Severino Da Dalt, Joan Llop, Irene Baucells, Marc Pamies, Yishi
               Xu, Aitor Gonzalez-Agirre, Marta Villegas. Proceedings of the
               2024 Joint International Conference on Computational Linguistics,
               Language Resources and Evaluation (LREC-COLING 2024). 2024.",
  year      =  2024
}

@INPROCEEDINGS{Ustun2020-zu,
  title     = "{UDapter}: Language adaptation for truly universal dependency
               parsing",
  author    = "{\"{U}}st{\"{u}}n, Ahmet and Bisazza, Arianna and Bouma, Gosse
               and van Noord, Gertjan",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in
               Natural Language Processing (EMNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "2302--2315",
  abstract  = "Ahmet {\"{U}}st{\"{u}}n, Arianna Bisazza, Gosse Bouma, Gertjan
               van Noord. Proceedings of the 2020 Conference on Empirical
               Methods in Natural Language Processing (EMNLP). 2020.",
  month     =  nov,
  year      =  2020
}

@ARTICLE{Cui2024-xu,
  title         = "Rethinking {LLM} language adaptation: A case study on Chinese
                   Mixtral",
  author        = "Cui, Yiming and Yao, Xin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Mixtral, a representative sparse mixture of experts (SMoE)
                   language model, has received significant attention due to its
                   unique model design and superior performance. Based on
                   Mixtral-8x7B-v0.1, in this paper, we propose Chinese-Mixtral
                   and Chinese-Mixtral-Instruct with improved Chinese language
                   abilities by adopting further pre-training and instruction
                   fine-tuning. Experimental results show that our
                   Chinese-Mixtral and Chinese-Mixtral-Instruct successfully
                   improve Chinese understanding and generation performance
                   while retaining the original English abilities. Then, we
                   discuss several key questions when performing language
                   adaptation on large language models, including the necessity
                   of extending the language-specific vocabulary and the choice
                   of the initialization model (foundation model v.s.
                   instruction model), by providing empirical results and
                   analysis. We also present the visualizations of each expert
                   to examine their importance on downstream tasks. Our
                   resources are publicly available through
                   \url{https://github.com/ymcui/Chinese-Mixtral}.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Conneau2018-cc,
  title     = "{XNLI}: Evaluating Cross-lingual Sentence Representations",
  author    = "Conneau, Alexis and Rinott, Ruty and Lample, Guillaume and
               Williams, Adina and Bowman, Samuel and Schwenk, Holger and
               Stoyanov, Veselin",
  booktitle = "Proceedings of the 2018 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "2475--2485",
  abstract  = "State-of-the-art natural language processing systems rely on
               supervision in the form of annotated data to learn competent
               models. These models are generally trained on data in a single
               language (usually English), and cannot be directly used beyond
               that language. Since collecting data in every language is not
               realistic, there has been a growing interest in cross-lingual
               language understanding (XLU) and low-resource cross-language
               transfer. In this work, we construct an evaluation set for XLU by
               extending the development and test sets of the Multi-Genre
               Natural Language Inference Corpus (MultiNLI) to 15 languages,
               including low-resource languages such as Swahili and Urdu. We
               hope that our dataset, dubbed XNLI, will catalyze research in
               cross-lingual sentence understanding by providing an informative
               standard evaluation task. In addition, we provide several
               baselines for multilingual sentence understanding, including two
               based on machine translation systems, and two that use parallel
               data to train aligned multilingual bag-of-words and LSTM
               encoders. We find that XNLI represents a practical and
               challenging evaluation suite, and that directly translating the
               test data yields the best performance among available baselines.",
  year      =  2018
}

@ARTICLE{Lai2023-ik,
  title         = "{ChatGPT} beyond English: Towards a comprehensive evaluation
                   of large language models in multilingual learning",
  author        = "Lai, Viet Dac and Ngo, Nghia Trung and Veyseh, Amir Pouran
                   Ben and Man, Hieu and Dernoncourt, Franck and Bui, Trung and
                   Nguyen, Thien Huu",
  journal       = "arXiv [cs.CL]",
  abstract      = "Over the last few years, large language models (LLMs) have
                   emerged as the most important breakthroughs in natural
                   language processing (NLP) that fundamentally transform
                   research and developments in the field. ChatGPT represents
                   one of the most exciting LLM systems developed recently to
                   showcase impressive skills for language generation and highly
                   attract public attention. Among various exciting applications
                   discovered for ChatGPT in English, the model can process and
                   generate texts for multiple languages due to its multilingual
                   training data. Given the broad adoption of ChatGPT for
                   English in different problems and areas, a natural question
                   is whether ChatGPT can also be applied effectively for other
                   languages or it is necessary to develop more
                   language-specific technologies. The answer to this question
                   requires a thorough evaluation of ChatGPT over multiple tasks
                   with diverse languages and large datasets (i.e., beyond
                   reported anecdotes), which is still missing or limited in
                   current research. Our work aims to fill this gap for the
                   evaluation of ChatGPT and similar LLMs to provide more
                   comprehensive information for multilingual NLP applications.
                   While this work will be an ongoing effort to include
                   additional experiments in the future, our current paper
                   evaluates ChatGPT on 7 different tasks, covering 37 diverse
                   languages with high, medium, low, and extremely low
                   resources. We also focus on the zero-shot learning setting
                   for ChatGPT to improve reproducibility and better simulate
                   the interactions of general users. Compared to the
                   performance of previous models, our extensive experimental
                   results demonstrate a worse performance of ChatGPT for
                   different NLP tasks and languages, calling for further
                   research to develop better models and understanding for
                   multilingual learning.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Asai2024-oe,
  title     = "{BUFFET}: Benchmarking large language models for few-shot
               cross-lingual transfer",
  author    = "Asai, Akari and Kudugunta, Sneha and Yu, Xinyan and Blevins,
               Terra and Gonen, Hila and Reid, Machel and Tsvetkov, Yulia and
               Ruder, Sebastian and Hajishirzi, Hannaneh",
  booktitle = "Proceedings of the 2024 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1771--1800",
  abstract  = "Akari Asai, Sneha Kudugunta, Xinyan Yu, Terra Blevins, Hila
               Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, Hannaneh
               Hajishirzi. Proceedings of the 2024 Conference of the North
               American Chapter of the Association for Computational
               Linguistics: Human Language Technologies (Volume 1: Long Papers).
               2024.",
  year      =  2024
}

@ARTICLE{Ahuja2023-lz,
  title         = "{MEGA}: Multilingual evaluation of generative {AI}",
  author        = "Ahuja, Kabir and Diddee, Harshita and Hada, Rishav and
                   Ochieng, Millicent and Ramesh, Krithika and Jain, Prachi and
                   Nambi, Akshay and Ganu, Tanuja and Segal, Sameer and Axmed,
                   Maxamed and Bali, Kalika and Sitaram, Sunayana",
  journal       = "arXiv [cs.CL]",
  abstract      = "Generative AI models have shown impressive performance on
                   many Natural Language Processing tasks such as language
                   understanding, reasoning, and language generation. An
                   important question being asked by the AI community today is
                   about the capabilities and limits of these models, and it is
                   clear that evaluating generative AI is very challenging. Most
                   studies on generative LLMs have been restricted to English
                   and it is unclear how capable these models are at
                   understanding and generating text in other languages. We
                   present the first comprehensive benchmarking of generative
                   LLMs - MEGA, which evaluates models on standard NLP
                   benchmarks, covering 16 NLP datasets across 70 typologically
                   diverse languages. We compare the performance of generative
                   LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA)
                   non-autoregressive models on these tasks to determine how
                   well generative models perform compared to the previous
                   generation of LLMs. We present a thorough analysis of the
                   performance of models across languages and tasks and discuss
                   challenges in improving the performance of generative LLMs on
                   low-resource languages. We create a framework for evaluating
                   generative LLMs in the multilingual setting and provide
                   directions for future progress in the field.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Shing2025-ky,
  title         = "{TAID}: Temporally Adaptive Interpolated Distillation for
                   efficient knowledge transfer in language models",
  author        = "Shing, Makoto and Misaki, Kou and Bao, Han and Yokoi, Sho and
                   Akiba, Takuya",
  journal       = "arXiv [cs.LG]",
  abstract      = "Causal language models have demonstrated remarkable
                   capabilities, but their size poses significant challenges for
                   deployment in resource-constrained environments. Knowledge
                   distillation, a widely-used technique for transferring
                   knowledge from a large teacher model to a small student
                   model, presents a promising approach for model compression. A
                   significant remaining issue lies in the major differences
                   between teacher and student models, namely the substantial
                   capacity gap, mode averaging, and mode collapse, which pose
                   barriers during distillation. To address these issues, we
                   introduce $\textit{Temporally Adaptive Interpolated
                   Distillation (TAID)}$, a novel knowledge distillation
                   approach that dynamically interpolates student and teacher
                   distributions through an adaptive intermediate distribution,
                   gradually shifting from the student's initial distribution
                   towards the teacher's distribution. We provide a theoretical
                   analysis demonstrating TAID's ability to prevent mode
                   collapse and empirically show its effectiveness in addressing
                   the capacity gap while balancing mode averaging and mode
                   collapse. Our comprehensive experiments demonstrate TAID's
                   superior performance across various model sizes and
                   architectures in both instruction tuning and pre-training
                   scenarios. Furthermore, we showcase TAID's practical impact
                   by developing two state-of-the-art compact foundation models:
                   $\texttt{TAID-LLM-1.5B}$ for language tasks and
                   $\texttt{TAID-VLM-2B}$ for vision-language tasks. These
                   results demonstrate TAID's effectiveness in creating
                   high-performing and efficient models, advancing the
                   development of more accessible AI technologies.",
  month         =  jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Ostendorff2023-uv,
  title         = "Efficient language model training through cross-lingual and
                   progressive transfer learning",
  author        = "Ostendorff, Malte and Rehm, Georg",
  journal       = "arXiv [cs.CL]",
  abstract      = "Most Transformer language models are primarily pretrained on
                   English text, limiting their use for other languages. As the
                   model sizes grow, the performance gap between English and
                   other languages with fewer compute and data resources
                   increases even further. Consequently, more resource-efficient
                   training methods are needed to bridge the gap for languages
                   with fewer resources available. To address this problem, we
                   introduce a cross-lingual and progressive transfer learning
                   approach, called CLP-Transfer, that transfers models from a
                   source language, for which pretrained models are publicly
                   available, like English, to a new target language. As opposed
                   to prior work, which focused on the cross-lingual transfer
                   between two languages, we extend the transfer to the model
                   size. Given a pretrained model in a source language, we aim
                   for a same-sized model in a target language. Instead of
                   training a model from scratch, we exploit a smaller model
                   that is in the target language but requires much fewer
                   resources. Both small and source models are then used to
                   initialize the token embeddings of the larger model based on
                   the overlapping vocabulary of the source and target language.
                   All remaining weights are reused from the model in the source
                   language. This approach outperforms the sole cross-lingual
                   transfer and can save up to 80\% of the training steps
                   compared to the random initialization.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Lample2019-fw,
  title         = "Cross-lingual Language Model Pretraining",
  author        = "Lample, Guillaume and Conneau, Alexis",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent studies have demonstrated the efficiency of generative
                   pretraining for English natural language understanding. In
                   this work, we extend this approach to multiple languages and
                   show the effectiveness of cross-lingual pretraining. We
                   propose two methods to learn cross-lingual language models
                   (XLMs): one unsupervised that only relies on monolingual
                   data, and one supervised that leverages parallel data with a
                   new cross-lingual language model objective. We obtain
                   state-of-the-art results on cross-lingual classification,
                   unsupervised and supervised machine translation. On XNLI, our
                   approach pushes the state of the art by an absolute gain of
                   4.9\% accuracy. On unsupervised machine translation, we
                   obtain 34.3 BLEU on WMT'16 German-English, improving the
                   previous state of the art by more than 9 BLEU. On supervised
                   machine translation, we obtain a new state of the art of 38.5
                   BLEU on WMT'16 Romanian-English, outperforming the previous
                   best approach by more than 4 BLEU. Our code and pretrained
                   models will be made publicly available.",
  month         =  jan,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Khant2025-ej,
  title         = "Should code models learn pedagogically? A preliminary
                   evaluation of Curriculum Learning for real-world software
                   engineering tasks",
  author        = "Khant, Kyi Shin and Lin, Hong Yi and Thongtanunam, Patanamon",
  journal       = "arXiv [cs.SE]",
  abstract      = "Learning-based techniques, especially advanced pre-trained
                   models for code have demonstrated capabilities in code
                   understanding and generation, solving diverse software
                   engineering (SE) tasks. Despite the promising results,
                   current training approaches may not fully optimize model
                   performance, as they typically involve learning from randomly
                   shuffled training data. Recent work shows that Curriculum
                   Learning (CL) can improve performance on code-related tasks
                   through incremental learning based on the difficulty of
                   synthetic code. Yet, the effectiveness of CL with
                   conventional difficulty measures in SE tasks remains largely
                   unexplored. In this study, we explore two conventional code
                   metrics: code length and cyclomatic complexity to determine
                   the difficulty levels. We investigate how the pre-trained
                   code model (CodeT5) learns under CL, through the tasks of
                   code clone detection and code summarization. Our empirical
                   study on the CodeXGLUE benchmark showed contrasting results
                   to prior studies, where the model exhibited signs of
                   catastrophic forgetting and shortcut learning. Surprisingly,
                   model performance saturates after only the first quartile of
                   training, potentially indicating a limit in the model's
                   representation capacity and/or the task's inherent
                   difficulty. Future work should further explore various CL
                   strategies with different code models across a wider range of
                   SE tasks for a more holistic understanding.",
  month         =  feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SE"
}

@ARTICLE{Chang2022-mw,
  title         = "The geometry of multilingual language model representations",
  author        = "Chang, Tyler A and Tu, Zhuowen and Bergen, Benjamin K",
  journal       = "arXiv [cs.CL]",
  abstract      = "We assess how multilingual language models maintain a shared
                   multilingual representation space while still encoding
                   language-sensitive information in each language. Using XLM-R
                   as a case study, we show that languages occupy similar linear
                   subspaces after mean-centering, evaluated based on causal
                   effects on language modeling performance and direct
                   comparisons between subspaces for 88 languages. The subspace
                   means differ along language-sensitive axes that are
                   relatively stable throughout middle layers, and these axes
                   encode information such as token vocabularies. Shifting
                   representations by language means is sufficient to induce
                   token predictions in different languages. However, we also
                   identify stable language-neutral axes that encode information
                   such as token positions and part-of-speech. We visualize
                   representations projected onto language-sensitive and
                   language-neutral axes, identifying language family and
                   part-of-speech clusters, along with spirals, toruses, and
                   curves representing token position information. These results
                   demonstrate that multilingual language models encode
                   information along orthogonal language-sensitive and
                   language-neutral axes, allowing the models to extract a
                   variety of features for downstream tasks and cross-lingual
                   transfer learning.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Yin2024-aa,
  title         = "{LoFiT}: Localized Fine-tuning on {LLM} representations",
  author        = "Yin, Fangcong and Ye, Xi and Durrett, Greg",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent work in interpretability shows that large language
                   models (LLMs) can be adapted for new tasks in a learning-free
                   way: it is possible to intervene on LLM representations to
                   elicit desired behaviors for alignment. For instance, adding
                   certain bias vectors to the outputs of certain attention
                   heads is reported to boost the truthfulness of models. In
                   this work, we show that localized fine-tuning serves as an
                   effective alternative to such representation intervention
                   methods. We introduce a framework called Localized
                   Fine-Tuning on LLM Representations (LoFiT), which identifies
                   a subset of attention heads that are most important for
                   learning a specific task, then trains offset vectors to add
                   to the model's hidden representations at those selected
                   heads. LoFiT localizes to a sparse set of heads (3\%-10\%)
                   and learns the offset vectors from limited training data,
                   comparable to the settings used for representation
                   intervention. For truthfulness and reasoning tasks, we find
                   that LoFiT's intervention vectors are more effective for LLM
                   adaptation than vectors from representation intervention
                   methods such as Inference-time Intervention. We also find
                   that the localization step is important: selecting a
                   task-specific set of attention heads can lead to higher
                   performance than intervening on heads selected for a
                   different task. Finally, across 7 tasks we study, LoFiT
                   achieves comparable performance to other parameter-efficient
                   fine-tuning methods such as LoRA, despite modifying 20x-200x
                   fewer parameters than these methods.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Dai2022-il,
  title     = "Knowledge neurons in pretrained transformers",
  author    = "Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang,
               Baobao and Wei, Furu",
  editor    = "Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "8493--8502",
  month     =  may,
  year      =  2022
}

@ARTICLE{Juneja2022-wx,
  title         = "Finding patterns in Knowledge Attribution for Transformers",
  author        = "Juneja, Jeevesh and Agarwal, Ritu",
  journal       = "arXiv [cs.CL]",
  abstract      = "We analyze the Knowledge Neurons framework for the
                   attribution of factual and relational knowledge to particular
                   neurons in the transformer network. We use a 12-layer
                   multi-lingual BERT model for our experiments. Our study
                   reveals various interesting phenomena. We observe that mostly
                   factual knowledge can be attributed to middle and higher
                   layers of the network($\ge 6$). Further analysis reveals that
                   the middle layers($6-9$) are mostly responsible for
                   relational information, which is further refined into actual
                   factual knowledge or the ``correct answer'' in the last few
                   layers($10-12$). Our experiments also show that the model
                   handles prompts in different languages, but representing the
                   same fact, similarly, providing further evidence for
                   effectiveness of multi-lingual pre-training. Applying the
                   attribution scheme for grammatical knowledge, we find that
                   grammatical knowledge is far more dispersed among the neurons
                   than factual knowledge.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Geva2020-rq,
  title         = "Transformer feed-forward layers are key-value memories",
  author        = "Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy,
                   Omer",
  journal       = "arXiv [cs.CL]",
  abstract      = "Feed-forward layers constitute two-thirds of a transformer
                   model's parameters, yet their role in the network remains
                   under-explored. We show that feed-forward layers in
                   transformer-based language models operate as key-value
                   memories, where each key correlates with textual patterns in
                   the training examples, and each value induces a distribution
                   over the output vocabulary. Our experiments show that the
                   learned patterns are human-interpretable, and that lower
                   layers tend to capture shallow patterns, while upper layers
                   learn more semantic ones. The values complement the keys'
                   input patterns by inducing output distributions that
                   concentrate probability mass on tokens likely to appear
                   immediately after each pattern, particularly in the upper
                   layers. Finally, we demonstrate that the output of a
                   feed-forward layer is a composition of its memories, which is
                   subsequently refined throughout the model's layers via
                   residual connections to produce the final output
                   distribution.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hao2020-si,
  title         = "Self-Attention Attribution: Interpreting Information
                   Interactions Inside Transformer",
  author        = "Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke",
  journal       = "arXiv [cs.CL]",
  abstract      = "The great success of Transformer-based models benefits from
                   the powerful multi-head self-attention mechanism, which
                   learns token dependencies and encodes contextual information
                   from the input. Prior work strives to attribute model
                   decisions to individual input features with different
                   saliency measures, but they fail to explain how these input
                   features interact with each other to reach predictions. In
                   this paper, we propose a self-attention attribution method to
                   interpret the information interactions inside Transformer. We
                   take BERT as an example to conduct extensive studies.
                   Firstly, we apply self-attention attribution to identify the
                   important attention heads, while others can be pruned with
                   marginal performance degradation. Furthermore, we extract the
                   most salient dependencies in each layer to construct an
                   attribution tree, which reveals the hierarchical interactions
                   inside Transformer. Finally, we show that the attribution
                   results can be used as adversarial patterns to implement
                   non-targeted attacks towards BERT.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Subramani2022-mc,
  title     = "Extracting latent steering vectors from pretrained language
               models",
  author    = "Subramani, Nishant and Suresh, Nivedita and Peters, Matthew",
  booktitle = "Findings of the Association for Computational Linguistics: ACL
               2022",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "566--581",
  abstract  = "Nishant Subramani, Nivedita Suresh, Matthew Peters. Findings of
               the Association for Computational Linguistics: ACL 2022. 2022.",
  year      =  2022
}

@INPROCEEDINGS{Wu2024-bn,
  title     = "Advancing parameter efficiency in fine-tuning via representation
               editing",
  author    = "Wu, Muling and Liu, Wenhao and Wang, Xiaohua and Li, Tianlong and
               Lv, Changze and Ling, Zixuan and JianHao, Zhu and Zhang, Cenyuan
               and Zheng, Xiaoqing and Huang, Xuanjing",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "13445--13464",
  abstract  = "Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv,
               Zixuan Ling, Zhu JianHao, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing
               Huang. Proceedings of the 62nd Annual Meeting of the Association
               for Computational Linguistics (Volume 1: Long Papers). 2024.",
  year      =  2024
}

@ARTICLE{Wu2024-gn,
  title         = "{ReFT}: Representation Finetuning for language models",
  author        = "Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger,
                   Atticus and Jurafsky, Dan and Manning, Christopher D and
                   Potts, Christopher",
  journal       = "arXiv [cs.CL]",
  abstract      = "Parameter-efficient finetuning (PEFT) methods seek to adapt
                   large neural models via updates to a small number of weights.
                   However, much prior interpretability work has shown that
                   representations encode rich semantic information, suggesting
                   that editing representations might be a more powerful
                   alternative. We pursue this hypothesis by developing a family
                   of Representation Finetuning (ReFT) methods. ReFT methods
                   operate on a frozen base model and learn task-specific
                   interventions on hidden representations. We define a strong
                   instance of the ReFT family, Low-rank Linear Subspace ReFT
                   (LoReFT), and we identify an ablation of this method that
                   trades some performance for increased efficiency. Both are
                   drop-in replacements for existing PEFTs and learn
                   interventions that are 15x--65x more parameter-efficient than
                   LoRA. We showcase LoReFT on eight commonsense reasoning
                   tasks, four arithmetic reasoning tasks, instruction-tuning,
                   and GLUE. In all these evaluations, our ReFTs deliver the
                   best balance of efficiency and performance, and almost always
                   outperform state-of-the-art PEFTs. We release a generic ReFT
                   training library publicly at
                   https://github.com/stanfordnlp/pyreft.",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Liu2024-ab,
  title     = "Unraveling babel: Exploring multilingual activation patterns of
               {LLMs} and their applications",
  author    = "Liu, Weize and Xu, Yinlong and Xu, Hongxia and Chen, Jintai and
               Hu, Xuming and Wu, Jian",
  editor    = "Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung",
  booktitle = "Proceedings of the 2024 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "11855--11881",
  month     =  nov,
  year      =  2024
}

@ARTICLE{Zhou2024-ba,
  title         = "On the role of attention heads in large language model safety",
  author        = "Zhou, Zhenhong and Yu, Haiyang and Zhang, Xinghua and Xu,
                   Rongwu and Huang, Fei and Wang, Kun and Liu, Yang and Fang,
                   Junfeng and Li, Yongbin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) achieve state-of-the-art
                   performance on multiple language tasks, yet their safety
                   guardrails can be circumvented, leading to harmful
                   generations. In light of this, recent research on safety
                   mechanisms has emerged, revealing that when safety
                   representations or component are suppressed, the safety
                   capability of LLMs are compromised. However, existing
                   research tends to overlook the safety impact of multi-head
                   attention mechanisms, despite their crucial role in various
                   model functionalities. Hence, in this paper, we aim to
                   explore the connection between standard attention mechanisms
                   and safety capability to fill this gap in the safety-related
                   mechanistic interpretability. We propose a novel metric which
                   tailored for multi-head attention, the Safety Head ImPortant
                   Score (Ships), to assess the individual heads' contributions
                   to model safety. Based on this, we generalize Ships to the
                   dataset level and further introduce the Safety Attention Head
                   AttRibution Algorithm (Sahara) to attribute the critical
                   safety attention heads inside the model. Our findings show
                   that the special attention head has a significant impact on
                   safety. Ablating a single safety head allows aligned model
                   (e.g., Llama-2-7b-chat) to respond to 16 times more harmful
                   queries, while only modifying 0.006\% of the parameters, in
                   contrast to the ~ 5\% modification required in previous
                   studies. More importantly, we demonstrate that attention
                   heads primarily function as feature extractors for safety and
                   models fine-tuned from the same base model exhibit
                   overlapping safety heads through comprehensive experiments.
                   Together, our attribution approach and findings provide a
                   novel perspective for unpacking the black box of safety
                   mechanisms within large models.",
  month         =  oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Li2024-is,
  title         = "Safety layers in aligned large language models: The key to
                   {LLM} security",
  author        = "Li, Shen and Yao, Liuyi and Zhang, Lan and Li, Yaliang",
  journal       = "arXiv [cs.CR]",
  abstract      = "Aligned LLMs are secure, capable of recognizing and refusing
                   to answer malicious questions. However, the role of internal
                   parameters in maintaining such security is not well
                   understood yet, further these models can be vulnerable to
                   security degradation when subjected to fine-tuning attacks.
                   To address these challenges, our work uncovers the mechanism
                   behind security in aligned LLMs at the parameter level,
                   identifying a small set of contiguous layers in the middle of
                   the model that are crucial for distinguishing malicious
                   queries from normal ones, referred to as ``safety layers``.
                   We first confirm the existence of these safety layers by
                   analyzing variations in input vectors within the model's
                   internal layers. Additionally, we leverage the over-rejection
                   phenomenon and parameters scaling analysis to precisely
                   locate the safety layers. Building on these findings, we
                   propose a novel fine-tuning approach, Safely
                   Partial-Parameter Fine-Tuning (SPPFT), that fixes the
                   gradient of the safety layers during fine-tuning to address
                   the security degradation. Our experiments demonstrate that
                   the proposed approach can significantly preserve LLM security
                   while maintaining performance and reducing computational
                   resources compared to full fine-tuning.",
  month         =  aug,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CR"
}

@ARTICLE{Wei2024-nx,
  title         = "Assessing the brittleness of safety alignment via pruning and
                   low-rank modifications",
  author        = "Wei, Boyi and Huang, Kaixuan and Huang, Yangsibo and Xie,
                   Tinghao and Qi, Xiangyu and Xia, Mengzhou and Mittal, Prateek
                   and Wang, Mengdi and Henderson, Peter",
  journal       = "arXiv [cs.LG]",
  abstract      = "Large language models (LLMs) show inherent brittleness in
                   their safety mechanisms, as evidenced by their susceptibility
                   to jailbreaking and even non-malicious fine-tuning. This
                   study explores this brittleness of safety alignment by
                   leveraging pruning and low-rank modifications. We develop
                   methods to identify critical regions that are vital for
                   safety guardrails, and that are disentangled from
                   utility-relevant regions at both the neuron and rank levels.
                   Surprisingly, the isolated regions we find are sparse,
                   comprising about $3\%$ at the parameter level and $2.5\%$ at
                   the rank level. Removing these regions compromises safety
                   without significantly impacting utility, corroborating the
                   inherent brittleness of the model's safety mechanisms.
                   Moreover, we show that LLMs remain vulnerable to low-cost
                   fine-tuning attacks even when modifications to the
                   safety-critical regions are restricted. These findings
                   underscore the urgent need for more robust safety strategies
                   in LLMs.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@MISC{noauthor_undated-ip,
  title = "Identifying and Tuning Safety Neurons in Large Language Models"
}

@INPROCEEDINGS{Tang2024-fd,
  title     = "Language-specific neurons: The key to multilingual capabilities
               in large language models",
  author    = "Tang, Tianyi and Luo, Wenyang and Huang, Haoyang and Zhang,
               Dongdong and Wang, Xiaolei and Zhao, Xin and Wei, Furu and Wen,
               Ji-Rong",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "5701--5715",
  abstract  = "Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei
               Wang, Xin Zhao, Furu Wei, Ji-Rong Wen. Proceedings of the 62nd
               Annual Meeting of the Association for Computational Linguistics
               (Volume 1: Long Papers). 2024.",
  year      =  2024
}

@ARTICLE{Zhao2024-bg,
  title         = "How do large language models handle multilingualism?",
  author        = "Zhao, Yiran and Zhang, Wenxuan and Chen, Guizhen and
                   Kawaguchi, Kenji and Bing, Lidong",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) have demonstrated impressive
                   capabilities across diverse languages. This study explores
                   how LLMs handle multilingualism. Based on observed language
                   ratio shifts among layers and the relationships between
                   network structures and certain capabilities, we hypothesize
                   the LLM's multilingual workflow ($\texttt{MWork}$): LLMs
                   initially understand the query, converting multilingual
                   inputs into English for task-solving. In the intermediate
                   layers, they employ English for thinking and incorporate
                   multilingual knowledge with self-attention and feed-forward
                   structures, respectively. In the final layers, LLMs generate
                   responses aligned with the original language of the query. To
                   verify $\texttt{MWork}$, we introduce Parallel
                   Language-specific Neuron Detection ($\texttt{PLND}$) to
                   identify activated neurons for inputs in different languages
                   without any labeled data. Using $\texttt{PLND}$, we validate
                   $\texttt{MWork}$ through extensive experiments involving the
                   deactivation of language-specific neurons across various
                   layers and structures. Moreover, $\texttt{MWork}$ allows
                   fine-tuning of language-specific neurons with a small
                   dataset, enhancing multilingual abilities in a specific
                   language without compromising others. This approach results
                   in an average improvement of $3.6\%$ for high-resource
                   languages and $2.3\%$ for low-resource languages across all
                   tasks with just $400$ documents.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Kojima2024-qb,
  title     = "On the multilingual ability of decoder-based pre-trained language
               models: Finding and controlling language-specific neurons",
  author    = "Kojima, Takeshi and Okimura, Itsuki and Iwasawa, Yusuke and
               Yanaka, Hitomi and Matsuo, Yutaka",
  booktitle = "Proceedings of the 2024 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "6919--6971",
  abstract  = "Takeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hitomi Yanaka,
               Yutaka Matsuo. Proceedings of the 2024 Conference of the North
               American Chapter of the Association for Computational
               Linguistics: Human Language Technologies (Volume 1: Long Papers).
               2024.",
  year      =  2024
}

@INPROCEEDINGS{Dufter2020-ry,
  title     = "Identifying elements essential for {BERT}'s multilinguality",
  author    = "Dufter, Philipp and Sch{\"{u}}tze, Hinrich",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in
               Natural Language Processing (EMNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "4423--4437",
  abstract  = "Philipp Dufter, Hinrich Sch{\"{u}}tze. Proceedings of the 2020
               Conference on Empirical Methods in Natural Language Processing
               (EMNLP). 2020.",
  month     =  nov,
  year      =  2020
}

@INPROCEEDINGS{Pires2023-oo,
  title     = "Learning language-specific layers for multilingual machine
               translation",
  author    = "Pires, Telmo and Schmidt, Robin and Liao, Yi-Hsiu and Peitz,
               Stephan",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "14767--14783",
  abstract  = "Telmo Pires, Robin Schmidt, Yi-Hsiu Liao, Stephan Peitz.
               Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers). 2023.",
  year      =  2023
}

@ARTICLE{Kirkpatrick2016-at,
  title         = "Overcoming catastrophic forgetting in neural networks",
  author        = "Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil
                   and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A
                   and Milan, Kieran and Quan, John and Ramalho, Tiago and
                   Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath,
                   Claudia and Kumaran, Dharshan and Hadsell, Raia",
  journal       = "arXiv [cs.LG]",
  abstract      = "The ability to learn tasks in a sequential fashion is crucial
                   to the development of artificial intelligence. Neural
                   networks are not, in general, capable of this and it has been
                   widely thought that catastrophic forgetting is an inevitable
                   feature of connectionist models. We show that it is possible
                   to overcome this limitation and train networks that can
                   maintain expertise on tasks which they have not experienced
                   for a long time. Our approach remembers old tasks by
                   selectively slowing down learning on the weights important
                   for those tasks. We demonstrate our approach is scalable and
                   effective by solving a set of classification tasks based on
                   the MNIST hand written digit dataset and by learning several
                   Atari 2600 games sequentially.",
  month         =  dec,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Gheini2021-ag,
  title         = "Cross-attention is all you need: Adapting pretrained
                   transformers for machine translation",
  author        = "Gheini, Mozhdeh and Ren, Xiang and May, Jonathan",
  journal       = "arXiv [cs.CL]",
  abstract      = "We study the power of cross-attention in the Transformer
                   architecture within the context of transfer learning for
                   machine translation, and extend the findings of studies into
                   cross-attention when training from scratch. We conduct a
                   series of experiments through fine-tuning a translation model
                   on data where either the source or target language has
                   changed. These experiments reveal that fine-tuning only the
                   cross-attention parameters is nearly as effective as
                   fine-tuning all parameters (i.e., the entire translation
                   model). We provide insights into why this is the case and
                   observe that limiting fine-tuning in this manner yields
                   cross-lingually aligned embeddings. The implications of this
                   finding for researchers and practitioners include a
                   mitigation of catastrophic forgetting, the potential for
                   zero-shot translation, and the ability to extend machine
                   translation models to several new language pairs with reduced
                   parameter storage overhead.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Dumas2024-mz,
  title         = "Separating tongue from thought: Activation patching reveals
                   language-agnostic concept representations in transformers",
  author        = "Dumas, Cl\'{e}ment and Wendler, Chris and Veselovsky,
                   Veniamin and Monea, Giovanni and West, Robert",
  journal       = "arXiv [cs.CL]",
  abstract      = "A central question in multilingual language modeling is
                   whether large language models (LLMs) develop a universal
                   concept representation, disentangled from specific languages.
                   In this paper, we address this question by analyzing latent
                   representations (latents) during a word translation task in
                   transformer-based LLMs. We strategically extract latents from
                   a source translation prompt and insert them into the forward
                   pass on a target translation prompt. By doing so, we find
                   that the output language is encoded in the latent at an
                   earlier layer than the concept to be translated. Building on
                   this insight, we conduct two key experiments. First, we
                   demonstrate that we can change the concept without changing
                   the language and vice versa through activation patching
                   alone. Second, we show that patching with the mean over
                   latents across different languages does not impair and
                   instead improves the models' performance in translating the
                   concept. Our results provide evidence for the existence of
                   language-agnostic concept representations within the
                   investigated models.",
  month         =  nov,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Xu2023-us,
  title     = "Language representation projection: Can we transfer factual
               knowledge across languages in multilingual language models?",
  author    = "Xu, Shaoyang and Li, Junzhuo and Xiong, Deyi",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "3692--3702",
  abstract  = "Shaoyang Xu, Junzhuo Li, Deyi Xiong. Proceedings of the 2023
               Conference on Empirical Methods in Natural Language Processing.
               2023.",
  month     =  dec,
  year      =  2023
}

@INPROCEEDINGS{Bhattacharya2023-jm,
  title     = "Unveiling multilinguality in transformer models: Exploring
               language specificity in feed-forward networks",
  author    = "Bhattacharya, Sunit and Bojar, Ond\v{r}ej",
  booktitle = "Proceedings of the 6th BlackboxNLP Workshop: Analyzing and
               Interpreting Neural Networks for NLP",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "120--126",
  abstract  = "Sunit Bhattacharya, Ond\v{r}ej Bojar. Proceedings of the 6th
               BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks
               for NLP. 2023.",
  month     =  dec,
  year      =  2023
}

@INPROCEEDINGS{Zhang2023-px,
  title     = "A closer look at transformer attention for multilingual
               translation",
  author    = "Zhang, Jingyi and de Melo, Gerard and Xu, Hongfei and Chen, Kehai",
  booktitle = "Proceedings of the Eighth Conference on Machine Translation",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "496--506",
  abstract  = "Jingyi Zhang, Gerard de Melo, Hongfei Xu, Kehai Chen. Proceedings
               of the Eighth Conference on Machine Translation. 2023.",
  month     =  dec,
  year      =  2023
}

@INPROCEEDINGS{Li2023-lj,
  title     = "Interpreting and exploiting functional specialization in
               multi-head attention under multi-task learning",
  author    = "Li, Chong and Wang, Shaonan and Zhang, Yunhao and Zhang, Jiajun
               and Zong, Chengqing",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "16460--16476",
  abstract  = "Chong Li, Shaonan Wang, Yunhao Zhang, Jiajun Zhang, Chengqing
               Zong. Proceedings of the 2023 Conference on Empirical Methods in
               Natural Language Processing. 2023.",
  month     =  dec,
  year      =  2023
}

@ARTICLE{Chang2023-jr,
  title         = "When is multilinguality a curse? Language modeling for 250
                   high- and low-resource languages",
  author        = "Chang, Tyler A and Arnett, Catherine and Tu, Zhuowen and
                   Bergen, Benjamin K",
  journal       = "arXiv [cs.CL]",
  abstract      = "Multilingual language models are widely used to extend NLP
                   systems to low-resource languages. However, concrete evidence
                   for the effects of multilinguality on language modeling
                   performance in individual languages remains scarce. Here, we
                   pre-train over 10,000 monolingual and multilingual language
                   models for over 250 languages, including multiple language
                   families that are under-studied in NLP. We assess how
                   language modeling performance in each language varies as a
                   function of (1) monolingual dataset size, (2) added
                   multilingual dataset size, (3) linguistic similarity of the
                   added languages, and (4) model size (up to 45M parameters).
                   We find that in moderation, adding multilingual data improves
                   low-resource language modeling performance, similar to
                   increasing low-resource dataset sizes by up to 33\%.
                   Improvements depend on the syntactic similarity of the added
                   multilingual data, with marginal additional effects of
                   vocabulary overlap. However, high-resource languages
                   consistently perform worse in multilingual pre-training
                   scenarios. As dataset sizes increase, adding multilingual
                   data begins to hurt performance for both low-resource and
                   high-resource languages, likely due to limited model capacity
                   (the ``curse of multilinguality''). These results suggest
                   that massively multilingual pre-training may not be optimal
                   for any languages involved, but that more targeted models can
                   significantly improve performance.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Qin2024-xt,
  title         = "Multilingual large language model: A survey of resources,
                   taxonomy and frontiers",
  author        = "Qin, Libo and Chen, Qiguang and Zhou, Yuhang and Chen, Zhi
                   and Li, Yinghui and Liao, Lizi and Li, Min and Che, Wanxiang
                   and Yu, Philip S",
  journal       = "arXiv [cs.CL]",
  abstract      = "Multilingual Large Language Models are capable of using
                   powerful Large Language Models to handle and respond to
                   queries in multiple languages, which achieves remarkable
                   success in multilingual natural language processing tasks.
                   Despite these breakthroughs, there still remains a lack of a
                   comprehensive survey to summarize existing approaches and
                   recent developments in this field. To this end, in this
                   paper, we present a thorough review and provide a unified
                   perspective to summarize the recent progress as well as
                   emerging trends in multilingual large language models (MLLMs)
                   literature. The contributions of this paper can be
                   summarized: (1) First survey: to our knowledge, we take the
                   first step and present a thorough review in MLLMs research
                   field according to multi-lingual alignment; (2) New taxonomy:
                   we offer a new and unified perspective to summarize the
                   current progress of MLLMs; (3) New frontiers: we highlight
                   several emerging frontiers and discuss the corresponding
                   challenges; (4) Abundant resources: we collect abundant
                   open-source resources, including relevant papers, data
                   corpora, and leaderboards. We hope our work can provide the
                   community with quick access and spur breakthrough research in
                   MLLMs.",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Stanczak2022-bz,
  title     = "Same neurons, different languages: Probing morphosyntax in
               multilingual pre-trained models",
  author    = "Stanczak, Karolina and Ponti, Edoardo and Torroba Hennigen, Lucas
               and Cotterell, Ryan and Augenstein, Isabelle",
  booktitle = "Proceedings of the 2022 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1589--1598",
  abstract  = "Karolina Stanczak, Edoardo Ponti, Lucas Torroba Hennigen, Ryan
               Cotterell, Isabelle Augenstein. Proceedings of the 2022
               Conference of the North American Chapter of the Association for
               Computational Linguistics: Human Language Technologies. 2022.",
  year      =  2022
}

@ARTICLE{Chen2023-en,
  title         = "Journey to the center of the knowledge neurons: Discoveries
                   of language-Independent Knowledge Neurons and Degenerate
                   Knowledge Neurons",
  author        = "Chen, Yuheng and Cao, Pengfei and Chen, Yubo and Liu, Kang
                   and Zhao, Jun",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pre-trained language models (PLMs) contain vast amounts of
                   factual knowledge, but how the knowledge is stored in the
                   parameters remains unclear. This paper delves into the
                   complex task of understanding how factual knowledge is stored
                   in multilingual PLMs, and introduces the Architecture-adapted
                   Multilingual Integrated Gradients method, which successfully
                   localizes knowledge neurons more precisely compared to
                   current methods, and is more universal across various
                   architectures and languages. Moreover, we conduct an in-depth
                   exploration of knowledge neurons, leading to the following
                   two important discoveries: (1) The discovery of
                   Language-Independent Knowledge Neurons, which store factual
                   knowledge in a form that transcends language. We design
                   cross-lingual knowledge editing experiments, demonstrating
                   that the PLMs can accomplish this task based on
                   language-independent neurons; (2) The discovery of Degenerate
                   Knowledge Neurons, a novel type of neuron showing that
                   different knowledge neurons can store the same fact. Its
                   property of functional overlap endows the PLMs with a robust
                   mastery of factual knowledge. We design fact-checking
                   experiments, proving that the degenerate knowledge neurons
                   can help the PLMs to detect wrong facts. Experiments
                   corroborate these findings, shedding light on the mechanisms
                   of factual knowledge storage in multilingual PLMs, and
                   contribute valuable insights to the field. The code is
                   available at https://github.com/heng840/AMIG.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Philippy2023-zt,
  title     = "Towards a common understanding of contributing factors for
               cross-lingual transfer in multilingual language models: A review",
  author    = "Philippy, Fred and Guo, Siwen and Haddadan, Shohreh",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "5877--5891",
  abstract  = "Fred Philippy, Siwen Guo, Shohreh Haddadan. Proceedings of the
               61st Annual Meeting of the Association for Computational
               Linguistics (Volume 1: Long Papers). 2023.",
  year      =  2023
}

@ARTICLE{Vaswani2017-iw,
  title         = "Attention is all you need",
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  journal       = "arXiv [cs.CL]",
  abstract      = "The dominant sequence transduction models are based on
                   complex recurrent or convolutional neural networks in an
                   encoder-decoder configuration. The best performing models
                   also connect the encoder and decoder through an attention
                   mechanism. We propose a new simple network architecture, the
                   Transformer, based solely on attention mechanisms, dispensing
                   with recurrence and convolutions entirely. Experiments on two
                   machine translation tasks show these models to be superior in
                   quality while being more parallelizable and requiring
                   significantly less time to train. Our model achieves 28.4
                   BLEU on the WMT 2014 English-to-German translation task,
                   improving over the existing best results, including ensembles
                   by over 2 BLEU. On the WMT 2014 English-to-French translation
                   task, our model establishes a new single-model
                   state-of-the-art BLEU score of 41.8 after training for 3.5
                   days on eight GPUs, a small fraction of the training costs of
                   the best models from the literature. We show that the
                   Transformer generalizes well to other tasks by applying it
                   successfully to English constituency parsing both with large
                   and limited training data.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Frankle2018-rx,
  title         = "The lottery ticket hypothesis: Finding sparse, trainable
                   neural networks",
  author        = "Frankle, Jonathan and Carbin, Michael",
  journal       = "arXiv [cs.LG]",
  abstract      = "Neural network pruning techniques can reduce the parameter
                   counts of trained networks by over 90\%, decreasing storage
                   requirements and improving computational performance of
                   inference without compromising accuracy. However,
                   contemporary experience is that the sparse architectures
                   produced by pruning are difficult to train from the start,
                   which would similarly improve training performance. We find
                   that a standard pruning technique naturally uncovers
                   subnetworks whose initializations made them capable of
                   training effectively. Based on these results, we articulate
                   the ``lottery ticket hypothesis:'' dense,
                   randomly-initialized, feed-forward networks contain
                   subnetworks (``winning tickets'') that - when trained in
                   isolation - reach test accuracy comparable to the original
                   network in a similar number of iterations. The winning
                   tickets we find have won the initialization lottery: their
                   connections have initial weights that make training
                   particularly effective. We present an algorithm to identify
                   winning tickets and a series of experiments that support the
                   lottery ticket hypothesis and the importance of these
                   fortuitous initializations. We consistently find winning
                   tickets that are less than 10-20\% of the size of several
                   fully-connected and convolutional feed-forward architectures
                   for MNIST and CIFAR10. Above this size, the winning tickets
                   that we find learn faster than the original network and reach
                   higher test accuracy.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Xu2024-ws,
  title         = "Let's focus on neuron: Neuron-level supervised fine-tuning
                   for large language model",
  author        = "Xu, Haoyun and Zhan, Runzhe and Wong, Derek F and Chao, Lidia
                   S",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large Language Models (LLMs) are composed of neurons that
                   exhibit various behaviors and roles, which become
                   increasingly diversified as models scale. Recent studies have
                   revealed that not all neurons are active across different
                   datasets, and this sparsity correlates positively with the
                   task-specific ability, leading to advancements in model
                   pruning and training efficiency. Traditional fine-tuning
                   methods engage all parameters of LLMs, which is
                   computationally expensive and may not be necessary. In
                   contrast, Parameter-Efficient Fine-Tuning (PEFT) approaches
                   aim to minimize the number of trainable parameters, yet they
                   still operate at a relatively macro scale (e.g.,
                   layer-level). We introduce Neuron-Level Fine-Tuning (NeFT), a
                   novel approach that refines the granularity of parameter
                   training down to the individual neuron, enabling more precise
                   and computationally efficient model updates. The experimental
                   results show that NeFT not only exceeded the performance of
                   full-parameter fine-tuning and PEFT but also provided
                   insights into the analysis of neurons.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{John2019-fg,
  title     = "Disentangled representation learning for non-parallel text style
               transfer",
  author    = "John, Vineet and Mou, Lili and Bahuleyan, Hareesh and Vechtomova,
               Olga",
  editor    = "Korhonen, Anna and Traum, David and M\`{a}rquez, Llu\'{\i}s",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "424--434",
  year      =  2019
}

@INPROCEEDINGS{Zhao2024-pb,
  title     = "Understanding and Enhancing Safety Mechanisms of {LLMs} via
               Safety-Specific Neuron",
  author    = "Zhao, Yiran and Zhang, Wenxuan and Xie, Yuxi and Goyal, Anirudh
               and Kawaguchi, Kenji and Shieh, Michael",
  booktitle = "The Thirteenth International Conference on Learning
               Representations",
  abstract  = "Safety alignment for large language models (LLMs) has become a
               critical issue due to their rapid progress. However, our
               understanding of effective safety mechanisms in LLMs remains
               limited, leading to safety alignment training that mainly focuses
               on improving optimization, data-level enhancement, or adding
               extra structures to intentionally block harmful outputs. To
               address this gap, we develop a neuron detection method to
               identify safety neurons--those consistently crucial for handling
               and defending against harmful queries. Our findings reveal that
               these safety neurons constitute less than $1\%$ of all
               parameters, are language-specific and are predominantly located
               in self-attention layers. Moreover, safety is collectively
               managed by these neurons in the first several layers. Based on
               these observations, we introduce a $\underline{S}$afety
               $\underline{N}$euron $\underline{Tun}$ing method, named
               $\texttt{SN-Tune}$, that exclusively tune safety neurons without
               compromising models' general capabilities. $\texttt{SN-Tune}$
               significantly enhances the safety of instruction-tuned models,
               notably reducing the harmful scores of Llama3-8B-Instruction from
               $65.5$ to $2.0$, Mistral-7B-Instruct-v0.2 from $70.8$ to $4.5$,
               and Vicuna-13B-1.5 from $93.5$ to $3.0$. Moreover,
               $\texttt{SN-Tune}$ can be applied to base models on efficiently
               establishing LLMs' safety mechanism. In addition, we propose
               $\underline{R}$obust $\underline{S}$afety $\underline{N}$euron
               $\underline{Tun}$ing method ($\texttt{RSN-Tune}$), which
               preserves the integrity of LLMs' safety mechanisms during
               downstream task fine-tuning by separating the safety neurons from
               models' foundation neurons.",
  month     =  oct,
  year      =  2024
}

@ARTICLE{Sun2023-ig,
  title         = "A simple and effective pruning approach for large language
                   models",
  author        = "Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J
                   Zico",
  journal       = "arXiv [cs.CL]",
  abstract      = "As their size increases, Large Languages Models (LLMs) are
                   natural candidates for network pruning methods: approaches
                   that drop a subset of network weights while striving to
                   preserve performance. Existing methods, however, require
                   either retraining, which is rarely affordable for
                   billion-scale LLMs, or solving a weight reconstruction
                   problem reliant on second-order information, which may also
                   be computationally expensive. In this paper, we introduce a
                   novel, straightforward yet effective pruning method, termed
                   Wanda (Pruning by Weights and activations), designed to
                   induce sparsity in pretrained LLMs. Motivated by the recent
                   observation of emergent large magnitude features in LLMs, our
                   approach prunes weights with the smallest magnitudes
                   multiplied by the corresponding input activations, on a
                   per-output basis. Notably, Wanda requires no retraining or
                   weight update, and the pruned LLM can be used as is. We
                   conduct a thorough evaluation of our method Wanda on LLaMA
                   and LLaMA-2 across various language benchmarks. Wanda
                   significantly outperforms the established baseline of
                   magnitude pruning and performs competitively against recent
                   method involving intensive weight update. Code is available
                   at https://github.com/locuslab/wanda.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Achille2017-bt,
  title         = "Critical learning periods in deep neural networks",
  author        = "Achille, Alessandro and Rovere, Matteo and Soatto, Stefano",
  journal       = "arXiv [cs.LG]",
  abstract      = "Similar to humans and animals, deep artificial neural
                   networks exhibit critical periods during which a temporary
                   stimulus deficit can impair the development of a skill. The
                   extent of the impairment depends on the onset and length of
                   the deficit window, as in animal models, and on the size of
                   the neural network. Deficits that do not affect low-level
                   statistics, such as vertical flipping of the images, have no
                   lasting effect on performance and can be overcome with
                   further training. To better understand this phenomenon, we
                   use the Fisher Information of the weights to measure the
                   effective connectivity between layers of a network during
                   training. Counterintuitively, information rises rapidly in
                   the early phases of training, and then decreases, preventing
                   redistribution of information resources in a phenomenon we
                   refer to as a loss of ``Information Plasticity''. Our
                   analysis suggests that the first few epochs are critical for
                   the creation of strong connections that are optimal relative
                   to the input data distribution. Once such strong connections
                   are created, they do not appear to change during additional
                   training. These findings suggest that the initial learning
                   transient, under-scrutinized compared to asymptotic behavior,
                   plays a key role in determining the outcome of the training
                   process. Our findings, combined with recent theoretical
                   results in the literature, also suggest that forgetting
                   (decrease of information in the weights) is critical to
                   achieving invariance and disentanglement in representation
                   learning. Finally, critical periods are not restricted to
                   biological systems, but can emerge naturally in learning
                   systems, whether biological or artificial, due to fundamental
                   constrains arising from learning dynamics and information
                   processing.",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Voita2019-of,
  title     = "The bottom-up evolution of representations in the transformer: A
               study with machine translation and language modeling objectives",
  author    = "Voita, Elena and Sennrich, Rico and Titov, Ivan",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in
               Natural Language Processing and the 9th International Joint
               Conference on Natural Language Processing (EMNLP-IJCNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "4396--4406",
  abstract  = "Elena Voita, Rico Sennrich, Ivan Titov. Proceedings of the 2019
               Conference on Empirical Methods in Natural Language Processing
               and the 9th International Joint Conference on Natural Language
               Processing (EMNLP-IJCNLP). 2019.",
  month     =  nov,
  year      =  2019
}
